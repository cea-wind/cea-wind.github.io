<!DOCTYPE html>





<html class="theme-next pisces use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    search: {
      root: '/',
      path: ''
    },
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="SEAWIND">
<meta property="og:url" content="http://cea-wind.github.io/index.html">
<meta property="og:site_name" content="SEAWIND">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SEAWIND">
  <link rel="canonical" href="http://cea-wind.github.io/">


<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>SEAWIND</title>
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <div class="container sidebar-position-left page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SEAWIND</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
    <ul id="menu" class="menu">
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
    </ul>
</nav>

</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cea-wind.github.io/2020/03/09/matrix_accel_design/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SEAWIND">
    </span>
      <header class="post-header">

        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2020/03/09/matrix_accel_design/" class="post-title-link" itemprop="url">矩阵乘法加速器的设计框架</a>
              
            
          </h2>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-03-09 21:12:52" itemprop="dateCreated datePublished" datetime="2020-03-09T21:12:52+08:00">2020-03-09</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-03-10 00:07:08" itemprop="dateModified" datetime="2020-03-10T00:07:08+08:00">2020-03-10</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI-Chip/" itemprop="url" rel="index"><span itemprop="name">AI Chip</span></a></span>

                
                
              
            </span>
          

          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <blockquote>
<p>以往我分析了一些AI加速器的设计，包括TPU，FSD，华为达芬奇等，无一例外都是从已经给出的设计出发，去分析其优缺点和应用范围。在之前的文章中，关于这些设计是如何完成的，其背后是否有一定设计原则和理念的内容均没有进行探讨。而这两点，实则是设计一个优秀的，可持续迭代的加速器的基础。本文将从矩阵加速器出发，通过一些简化的模型，给出简单的设计框架。</p>
</blockquote>
<h1 id="1-矩阵乘法和硬件模型"><a href="#1-矩阵乘法和硬件模型" class="headerlink" title="1. 矩阵乘法和硬件模型"></a>1. 矩阵乘法和硬件模型</h1><p>一般来说，矩阵乘法加速器中需要加速的计算可表示为</p>
<p>$$ C = A\times B + C $$</p>
<p>其中$A\in R^{m\times k}$, $B\in R^{k\times n}$, $C\in R^{m\times n}$ 。</p>
<p>矩阵乘法加速器，一般至少包括计算单元，缓存（SRAM等构成）和内存（譬如DDR等）。其中缓存的读写速率较高，可以和计算单元的运算速度相匹配，但容量较小；内存的容量相对缓存较大，但读写速率较低。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20200309214550.png" alt></p>
<h1 id="2-带宽优化的矩阵乘法加速器设计"><a href="#2-带宽优化的矩阵乘法加速器设计" class="headerlink" title="2. 带宽优化的矩阵乘法加速器设计"></a>2. 带宽优化的矩阵乘法加速器设计</h1><p>和一般的处理器相比，特定的加速器可以设计数量巨大的计算单元（譬如Google TPU V1设计了65536个乘法器）；但是DDR的带宽的提升却是有限的。因此，设计目标之一在于优化数据访问，降低DDR的读写带宽。</p>
<p>假设加速器的总缓存大小为$M$, 在一次计算过程中，用于存储矩阵$A,B,C$的缓存空间大小分别为$M_A,M_B,M_C$。</p>
<p>矩阵乘法加速器的设计目的一般是为了加速大规模的矩阵乘法计算，为了简化分析过程，假设矩阵$A,B,C$的大小$S_A,S_B,S_C$均远大于$M$，即计算过程中每次只能在缓存中存放一部分数据，完成子矩阵$A_{sub},B_{sub},C_{sub}$的计算。显然，存放在缓存中的数据都会参与运算，否在有冗余数据浪费存储和带宽。因此$A_{sub},B_{sub},C_{sub}$应能够完成一组矩阵计算，即</p>
<p>$$A_{sub}\in R^{p\times s},B_{sub}\in R^{s\times q},C_{sub}\in R^{p\times q}$$</p>
<p>据此，为了完成矩阵计算，从DDR到SRAM的总数据读写为</p>
<p>$$D_{size} = n/q \times S_A + m/p \times S_B + 2\times S_C$$</p>
<p>据此可以给出优化目标为</p>
<p>$$<br>\mathbf{min} : mnk/q + mnk/p +2mn \ <br>\mathbf{sub.to }: p\times s + s\times q + p\times q \leqslant M\ <br> p&gt;0,s&gt;0,q&gt;0<br>$$</p>
<p>简化为</p>
<p>$$<br>\mathbf{min} : 1/q + 1/p \ <br>\mathbf{sub.to }: p\times s + s\times q + p\times q \leqslant M\ <br> p&gt;0,s&gt;0,q&gt;0<br>$$</p>
<p>求解得当$s=1$，$p=q=\sqrt{M+1}-1$时得到最优解。即若要设计一个带宽优化的乘法器，应该尽可能的将缓存用于存储$C_{sub}$，每次计算的子矩阵为</p>
<p>$$C_{sub}^{p\times q} += A_{sub}^{p\times 1}  + B_{sub}^{1\times q} $$</p>
<p>Telsa的FSD的设计和上述讨论结果是一致的（只不过FSD的SRAM对应了上述的DDR，Register对应了上述的SRAM），FSD计算过程中$A_{sub}\in R^{96\times 1},B_{sub}\in R^{96\times 96},C_{sub}\in R^{96\times 96}$。对应的FSD的设计实际上是以降低SRAM-Register之间的读写为目的进行优化的。</p>
<h1 id="3-计算优化的矩阵乘法加速器设计"><a href="#3-计算优化的矩阵乘法加速器设计" class="headerlink" title="3. 计算优化的矩阵乘法加速器设计"></a>3. 计算优化的矩阵乘法加速器设计</h1><p>依据第二节的结果，每次计算的子矩阵为</p>
<p>$$C_{sub}^{p\times q} += A_{sub}^{p\times 1}  + B_{sub}^{1\times q} $$</p>
<p>整个计算过程中，其并行度最高为${p\times q}$（即每个周期完成${p\times q}$个乘法）。而为了完成一次计算，需要从缓存里读取$p+q+q\times q$个数据送入到计算阵列中。因此一次读/写的数据位宽宽度极高，随着并行度的增长，数据位宽线性增长。</p>
<p>数据位宽的问题主要存在$C_{sub}$上。为了解决这一问题，Telsa FSD采用了移位的方式，在计算完成后，将计算结果依次写回到SRAM中。</p>
<p>如果设计目的在于计算阵列和缓存之间的优化，参考第二节的设计思路，在一定并行度上，希望尽可能降低缓存的读写带宽，优化目标可以表示为</p>
<p>$$<br>\mathbf{min}:x\times y+y\times z+x\times z \<br>\mathbf{sub.to }:x\times y\times z=P \ <br> x&gt;0,y&gt;0,z&gt;0<br>$$</p>
<p>其中$P$代表计算阵列的并行度，求解得当$x=y=z=\sqrt[3]{P}$时，此时设计的计算阵列对缓存的访问可以尽可能的低。</p>
<p>华为的达芬奇架构中计算阵列的设计和上述讨论是一致的，达芬奇中的CUBE Core是一个$16\times16\times16$的MAC阵列（以Davinci Max为例），可以完成<br>$$C_{sub}^{16\times 16} += A_{sub}^{16\times 16}  + B_{sub}^{16\times 16} $$<br>的矩阵计算。</p>
<h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h1><p>上述的所有讨论都基于一个最简单的硬件模型，从两个角度分别求解了理论上最优的设计应该是怎么样的。</p>
<p>实际情况往往会复杂很多，硬件架构方面就会复杂很多。同时优化的目标往往有多个，而优化的限制条件也会有很多。</p>
<p>但是在我看来，只有采用这样的设计方法，即将问题建模，求解，才能造就一个好的设计。也只有采用这样的设计方法，才能再已有的基础上，进一步增加优化目标和优化条件，进一步的优化架构设计。</p>

          
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
    </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cea-wind.github.io/2019/10/28/nvdla_winograd_conv/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SEAWIND">
    </span>
      <header class="post-header">

        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/10/28/nvdla_winograd_conv/" class="post-title-link" itemprop="url">NVDLA中Winograd卷积的设计</a>
              
            
          </h2>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-10-28 22:30:00" itemprop="dateCreated datePublished" datetime="2019-10-28T22:30:00+08:00">2019-10-28</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-03-10 00:32:40" itemprop="dateModified" datetime="2020-03-10T00:32:40+08:00">2020-03-10</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI-Chip/" itemprop="url" rel="index"><span itemprop="name">AI Chip</span></a></span>

                
                
              
            </span>
          

          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <blockquote>
<p>在<a href="http://localhost:4000/2019/08/21/ata_reuse_in_hpconv/" target="_blank" rel="noopener">AI芯片：高性能卷积计算中的数据复用</a>曾提到，基于变换域的卷积计算——譬如Winograd卷积——并不能适应算法上对卷积计算多变的需求。但Winograd卷积依旧出现在刚刚公开的ARM Ethos-N57和Ethos-N37 NPUs的支持特性中，本文将利用Nvidia开源的NVIDIA Deep Learning Accelerator (NVDLA)为例，分析在硬件中支持Winograd卷积的实现方式，代价和收益；以期对基于变换域卷积的优势和不足有更深的认识。</p>
</blockquote>
<h1 id="1-Windgrad卷积的计算方式"><a href="#1-Windgrad卷积的计算方式" class="headerlink" title="1. Windgrad卷积的计算方式"></a>1. Windgrad卷积的计算方式</h1><p>卷积神经网络中的三维卷积（后文简称为卷积）计算过程可以表示如下，将这种直接通过原始定义计算卷积的方式称为直接卷积（Direct Convolution）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for i = 1 : Ho</span><br><span class="line">    for j = 1 : Wo</span><br><span class="line">        for k = 1 : Co</span><br><span class="line">            for l = 1 : R</span><br><span class="line">                for m = 1 : S</span><br><span class="line">                    for n = 1 : Ci</span><br><span class="line">                        out[i,j,k] += In[i*s+l.j*s+m,n]*F[l,m,n];</span><br></pre></td></tr></table></figure>

<p>其中各参数的含义如下表</p>
<table>
<thead>
<tr>
<th>数据维度</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>Ho/Wo</td>
<td>输出feature map的高和宽</td>
</tr>
<tr>
<td>Co</td>
<td>输出的channel数目</td>
</tr>
<tr>
<td>R/S</td>
<td>filter的高和宽</td>
</tr>
<tr>
<td>Ci</td>
<td>输入的channel数目</td>
</tr>
<tr>
<td>s</td>
<td>卷积计算的stride</td>
</tr>
</tbody></table>
<p>和一般的乘加运算不同，卷积计算中有滑窗的过程，充分利用这一点特性可以节约计算过程中的乘法次数。关于Winograd的原理和推导，可以参考<a href="https://blog.csdn.net/antkillerfarm/article/details/78769624" target="_blank" rel="noopener">https://blog.csdn.net/antkillerfarm/article/details/78769624</a>中的相关内容。此处直接给出3x3, stride=1卷积下Winograd卷积的形式(参见<a href="http://nvdla.org/hw/v1/ias/unit_description.html" target="_blank" rel="noopener">NVDLA Unit</a>)。<br>$$S = A^T\left[\left(GgG^T\right) \odot \left( C^TdC \right) \right]A $$</p>
<p>$$<br>g = \begin{bmatrix}<br>wt_{0,0} &amp; wt_{0,1} &amp; wt_{0,2} \<br>wt_{1,0} &amp; wt_{1,1} &amp; wt_{1,2} \ <br>wt_{2,0} &amp; wt_{2,1} &amp; wt_{2,2}<br>\end{bmatrix}<br>$$</p>
<p>$$d = \begin{bmatrix}<br>x_{0,0} &amp; x_{0,1} &amp; x_{0,2}  &amp; x_{0,3}\ <br>x_{1,0} &amp; x_{1,1} &amp; x_{1,2}  &amp; x_{1,3}\ <br>x_{2,0} &amp; x_{2,1} &amp; x_{2,2}  &amp; x_{2,3}\ <br>x_{3,0} &amp; x_{3,1} &amp; x_{3,2}  &amp; x_{3,3}<br>\end{bmatrix}<br>$$</p>
<p>$$C = \begin{bmatrix}<br>1 &amp; 0 &amp; 0  &amp; 0 \ <br>0 &amp; 1 &amp; -1 &amp; 1 \ <br>-1&amp; 1 &amp; 1  &amp; 0 \ <br>0 &amp; 0 &amp; 0  &amp; -1<br>\end{bmatrix}<br>$$</p>
<p>$$G = \begin{bmatrix}<br>1   &amp; 0   &amp; 0  \ <br>0.5 &amp; 0.5 &amp; 0.5 \ <br>0.5 &amp; -0.5&amp; 0.5  \ <br>0   &amp; 0   &amp; 1<br>\end{bmatrix}<br>$$</p>
<p>$$A_T = \begin{bmatrix}<br>1 &amp; 1 &amp; 1 &amp; 0 \ <br>0 &amp; 1 &amp;-1 &amp;-1<br>\end{bmatrix}<br>$$</p>
<p>其中$g$是3x3的kernel，$d$是4x4的feature map，$\odot$表示矩阵对应位置元素相乘。$s$表示2x2的卷积结果。矩阵$C$, $G$, $A$为常量，用于Wingrad卷积中的变换。由于$C$, $G$, $A$中各元素取值为$\pm1,\pm0.5$, 因此计算可以通过加减和简单移位得到，认为不需要进行乘法运算。<br>因此，采用Winograd卷积计算得到4哥输出结果需要16次乘法计算，而直接卷积需要36次乘法计算。但是由于Winograd在变换中加入了加法计算，因此加法次数会有一定增加。注意上述讨论中并没有加入Channel方向，这是因为此处卷积在Channel上实际上依旧退化成了简单的乘加运算，因此无论在变换前后进行Channel方向计算均没有区别。</p>
<p>一段直接卷积和Winograd卷积对比的代码如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">g = np.random.randint(<span class="number">-128</span>,<span class="number">127</span>,(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">d = np.random.randint(<span class="number">-128</span>,<span class="number">127</span>,(<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line">direct_conv = np.zeros((<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">                direct_conv[i,j] =  direct_conv[i,j] + d[i+r,j+s]*g[r,s]</span><br><span class="line"></span><br><span class="line">C = np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">-1</span>,<span class="number">1</span>],[<span class="number">-1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">-1</span>]])</span><br><span class="line">G = np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>],[<span class="number">0.5</span>,<span class="number">-0.5</span>,<span class="number">0.5</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line">AT = np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">-1</span>,<span class="number">-1</span>]])</span><br><span class="line">U = G.dot(g).dot(G.transpose())</span><br><span class="line">V = C.transpose().dot(d).dot(C)</span><br><span class="line">wg_conv = AT.dot(U*V).dot(AT.transpose())</span><br><span class="line"></span><br><span class="line">print(direct_conv)</span><br><span class="line">print(wg_conv)</span><br></pre></td></tr></table></figure>

<p>由计算结果可知，两者结果完全一致（如果采用浮点数时可能会有量化误差，但都在合理范围内）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; <span class="built_in">print</span>(direct_conv)</span><br><span class="line">[[-23640.    -51.]</span><br><span class="line"> [-10740.   8740.]]</span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span>(wg_conv)</span><br><span class="line">[[-23640.    -51.]</span><br><span class="line"> [-10740.   8740.]]</span><br></pre></td></tr></table></figure>

<h1 id="2-NVDLA中的的直接卷积"><a href="#2-NVDLA中的的直接卷积" class="headerlink" title="2. NVDLA中的的直接卷积"></a>2. NVDLA中的的直接卷积</h1><p>在硬件设计过程中不可能为直接卷积和Winograd卷积分别设计完全独立的计算和控制逻辑，由于直接卷积有计算灵活和适应性强的特点，各类神经网络加速器都有支持。因此，Winograd一定是建立在直接卷积硬件结构基础上的拓展功能。在探究NVDLA中的Winograd卷积设计之前，必须先明确NVDLA中的的直接卷积的计算方式。<br>Nvidia的相关文档中十分详细的NVDLA计算直接卷积的流程（<a href="http://nvdla.org/hw/v1/ias/unit_description.html" target="_blank" rel="noopener">NVDLA Unit</a>)，其将卷积计算分成了五级（下述描述中，以数值精度为Int16为例）</p>
<ul>
<li>Atomic Operation （原子操作，完成16次64次乘法并将其加在一起）</li>
<li>Stripe Operation （条带操作，完成16次独立的Atomic Operation）</li>
<li>Block Operation  （块操作，完成kernel的R/S方向的累加）</li>
<li>Channel Operation（通道操作，完成Channel方向计算的累加）</li>
<li>Group Operation  （分组操作，完成一组kernel的全部计算）</li>
</ul>
<p><a href="http://nvdla.org/hw/v1/ias/unit_description.html" target="_blank" rel="noopener">NVDLA Unit</a>中给出了可视化的图像用于描述这个过程，这一过程实际上就是卷积的六层循环计算过程的拆解，可以表示如下</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> k = <span class="number">1</span> : Co/<span class="number">16</span></span><br><span class="line">    <span class="keyword">for</span> i = <span class="number">1</span> : Ho/<span class="number">4</span>                                <span class="comment">// Group Operation</span></span><br><span class="line">        <span class="keyword">for</span> j = <span class="number">1</span> : Wo/<span class="number">4</span>                            <span class="comment">// Group Operation</span></span><br><span class="line">            <span class="keyword">for</span> n = <span class="number">1</span> : Ci/<span class="number">64</span>                       <span class="comment">// Channel Operation</span></span><br><span class="line">                <span class="keyword">for</span> l = <span class="number">1</span> : R                       <span class="comment">// Block Operation</span></span><br><span class="line">                    <span class="keyword">for</span> m = <span class="number">1</span> : S                   <span class="comment">// Block Operation</span></span><br><span class="line">                        <span class="keyword">for</span> ii = <span class="number">1</span>:<span class="number">4</span>                <span class="comment">// Strip Operation</span></span><br><span class="line">                            <span class="keyword">for</span> ji = <span class="number">1</span>:<span class="number">4</span>            <span class="comment">// Strip Operation</span></span><br><span class="line">                                <span class="keyword">for</span> ki = <span class="number">1</span>：<span class="number">16</span>      <span class="comment">// Antomic Operation</span></span><br><span class="line">                                    <span class="keyword">for</span> ni = <span class="number">1</span>：<span class="number">64</span>  <span class="comment">// Antomic Block</span></span><br><span class="line">                                        out[i*<span class="number">4</span>+ii,j*<span class="number">4</span>+jj,k*<span class="number">16</span>+ki] += </span><br><span class="line">                                        In[(i*<span class="number">4</span>+ii)*s+l.(j*<span class="number">4</span>+jj)*s+m,n*<span class="number">64</span>+ni]*F[l,m,n*<span class="number">64</span>+ni];</span><br></pre></td></tr></table></figure>

<p>其中，Atomic Operation决定了NVDLA乘法阵列的设计。根据设计可以看出，NVDLA有16份完全一致的乘法阵列用于计算16个不同Kernel的乘法；而每个乘法阵列中有64个乘法和一棵64输入的加法树。<br>计算顺序还一定程度确定了NVDLA的Buffer设计和数据路径设计。在计算直接卷积时，每周期需要128Byte的Feature/Pixel数据，实际上时规则的64Channel的数据；因此在存储时只需要每个Bank上存储64Channel数据，使用时通过MUX选出指定Bank数据即可。在进行结果写回时，每周期需要写回16个Feature数据。由于Winograd卷积使用的Weight可以提前算好，对比直接卷积和Winograd卷积时可以忽略Weight路径。</p>
<h1 id="3-NVDLA中的Winograd卷积"><a href="#3-NVDLA中的Winograd卷积" class="headerlink" title="3. NVDLA中的Winograd卷积"></a>3. NVDLA中的Winograd卷积</h1><p>建立在直接卷积的硬件架构上，NVDLA针对Winograd卷积进行了一系列的修改。从计算方式上来说，不再同时计算64个Channel的乘加；从硬件架构上来说，进行了计算修改和数据路径修改。根据NVDLA的设计，Winograd卷积的计算$S = A^T\left[\left(GgG^T\right) \odot \left( C^TdC \right) \right]A$ 实际上分布在不同的阶段/模块进行。</p>
<ul>
<li>$U = GgG^T $是离线预先计算好的</li>
<li>$V = C^TdC $是在数据路径上计算的</li>
<li>$S = A^T\left[ U\odot V\right]A$ 是在计算阵列中计算的</li>
</ul>
<p>首先考虑计算阵列的设计。NVDLA计算3x3卷积，每次输出2x2共计4个数，计算过程中有4x4的矩阵点乘计算；结合直接卷积中64个乘法计算，Winograd卷积同时计算了4个Channel，共计4x4x4=64次乘法。乘法计算本身没有区别，但在进行加法时，和直接卷积略有不同，用代码可表示为</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//direct conv &amp; winograd conv</span></span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>:<span class="number">16</span></span><br><span class="line">    s1[i] = s0[i*<span class="number">4</span>+<span class="number">0</span>] + s0[i*<span class="number">4</span>+<span class="number">1</span>] + s0[i*<span class="number">4</span>+<span class="number">2</span>] + s0[i*<span class="number">4</span>+<span class="number">3</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">//direct conv</span></span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>:<span class="number">8</span></span><br><span class="line">    s2[i] = s1[i*<span class="number">2</span>+<span class="number">0</span>] + s1[i*<span class="number">2</span>+<span class="number">1</span>];</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>:<span class="number">4</span></span><br><span class="line">    s3[i] = s2[i*<span class="number">2</span>+<span class="number">0</span>] + s2[i*<span class="number">2</span>+<span class="number">1</span>];</span><br><span class="line">s4[i] = s3[<span class="number">0</span>] + s3[<span class="number">1</span>] + s3[<span class="number">2</span>] + s3[<span class="number">3</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">//winograd conv</span></span><br><span class="line"><span class="keyword">for</span> i=<span class="number">1</span>:<span class="number">4</span></span><br><span class="line">    s2_wg[<span class="number">0</span>][i] = s1[i*<span class="number">4</span>+<span class="number">0</span>] + s1[i*<span class="number">4</span>+<span class="number">1</span>] + s1[i*<span class="number">4</span>+<span class="number">2</span>];</span><br><span class="line">    s2_wg[<span class="number">0</span>][i] = s1[i*<span class="number">4</span>+<span class="number">1</span>] - s1[i*<span class="number">4</span>+<span class="number">2</span>] + s1[i*<span class="number">4</span>+<span class="number">3</span>];</span><br><span class="line">s3_wg[<span class="number">0</span>][<span class="number">0</span>] = s2_wg[<span class="number">0</span>][<span class="number">0</span>] + s2_wg[<span class="number">0</span>][<span class="number">1</span>] + s2_wg[<span class="number">0</span>][<span class="number">2</span>];</span><br><span class="line">s3_wg[<span class="number">1</span>][<span class="number">0</span>] = s2_wg[<span class="number">1</span>][<span class="number">0</span>] + s2_wg[<span class="number">1</span>][<span class="number">1</span>] + s2_wg[<span class="number">1</span>][<span class="number">2</span>];</span><br><span class="line">s3_wg[<span class="number">0</span>][<span class="number">1</span>] = s2_wg[<span class="number">0</span>][<span class="number">1</span>] - s2_wg[<span class="number">0</span>][<span class="number">1</span>] - s2_wg[<span class="number">0</span>][<span class="number">2</span>];</span><br><span class="line">s3_wg[<span class="number">1</span>][<span class="number">1</span>] = s2_wg[<span class="number">1</span>][<span class="number">1</span>] - s2_wg[<span class="number">1</span>][<span class="number">1</span>] - s2_wg[<span class="number">1</span>][<span class="number">2</span>];</span><br></pre></td></tr></table></figure>

<p>代码中只有第一级的加法被direct conv和winograd conv完全复用，其他级的加法略有不同。在NVDLA中，加法是使用Wallace Tree完成的，以提高性能降低资源占用。Direct Conv中和Winograd Conv中的后面几级加法还进行了进一步复用。总体来说，从代码上看（参见NV_NVDLA_CMAC_CORE_mac.v），为了支持Winograd卷积</p>
<ul>
<li>加法的第三级中增加了4棵4-2的Wallace Tree Compressor</li>
<li>加法的第四级中增加了2棵4-2的Wallace Tree Compressor</li>
<li>加法的第五级中增加了2棵6-2的Wallace Tree Compressor</li>
<li>增加了一些MUX以direct conv和winograd conv</li>
</ul>
<p>其次考虑数据路径，包括读取的数据路径和写回的数据路径。对于读取而言，除了需要针对Winograd专门设计取址逻辑和数据选择逻辑，还需要完成$V = C^TdC $的计算；根据文档描述，这一计算过程是在PRA（Pre-addition）中完成的。从代码上看（参见NV_NVDLA_CSC_dl.v）</p>
<ul>
<li>针对Winograd的地址生成增加的控制逻辑可以忽略</li>
<li>针对Winograd的数据选择增加数千的寄存器</li>
<li>PRA采用MENTOR的HLS综合工具实现，共实现了4份，和MAC阵列（1024乘加）对比，此处的计算资源较少</li>
</ul>
<p>对于写回路径而言，为了完成卷积计算，在乘加后增加了累加器和SRAM，其设计如下图所示(ref. <a href="http://nvdla.org/_images/ias_image21_cacc.png" target="_blank" rel="noopener">http://nvdla.org/_images/ias_image21_cacc.png</a>)</p>
<p><img src="http://nvdla.org/_images/ias_image21_cacc.png" alt></p>
<p>和Direct Conv一次输出16个结果相比，Winograd Conv输出的结果为64。这意味着为了支持Winograd Conv，需要额外增加48组高位宽的累加器。同时，SRAM的大小也需要设置为原先的四倍。</p>
<h1 id="4-相关讨论"><a href="#4-相关讨论" class="headerlink" title="4. 相关讨论"></a>4. 相关讨论</h1><p>NVDLA为了同时支持Direct Conv和Winograd Conv显然付出了一些代价。定性的分析来看，包括</p>
<ul>
<li>4组PRA，每组PRA中约有8次加法</li>
<li>16棵加法树，每棵增加了约8次加法</li>
<li>48组高位宽加法</li>
<li>增加了约25KB的Accumulator SRAM</li>
</ul>
<p>而作为对比，一些典型数据包括</p>
<ul>
<li>MAC阵列中有1024次乘法和约1024次加法</li>
<li>用于存放Feature/Pixel/Weight的Buffer大小为512KB</li>
</ul>
<p>显然，为了支持Winograd Conv增加的资源并不会太多。当然，虽然读取路径和计算阵列的设计受Winograd Conv的影响不大；但是对于写回路径而言，数据位宽发生了变化，一定程度影响了整体的架构设计。可能可以优化的地方包括将Direct Conv的输出也改成2x2的大小，这样写回的数据路径上Direct Conv和Winograd Conv就没有差别了。</p>
<p>NVDLA是一个相对专用的加速器，从相关文档中也可以看出，NVDLA专门针对计算中的各种特性/数据排列进行了硬件上的处理。而现有的很多加速器，为了兼顾不同网络的计算效率，往往更为灵活。在这种情况下，Winograd Conv应该作为设计的可选项，这是因为</p>
<ul>
<li>计算3x3卷积有2.25x的理论提升</li>
<li>Winograd Conv的乘法依旧是矩阵计算</li>
<li>Winograd Conv的数据路径和直接卷积没有必然的冲突</li>
<li>Winograd Conv的加法可以直接在数据路径上完成，甚至不影响其他设计</li>
<li>如果加速器设计粒度足够细，甚至可以从软件调度上直接支持Winograd Conv</li>
</ul>
<p>完全不考虑Winograd Conv的理由只可能是未来算法发展趋势下，3x3的普通卷积计算量占比会大大下降。</p>
<h1 id="5-参考"><a href="#5-参考" class="headerlink" title="5. 参考"></a>5. 参考</h1><ol>
<li><a href="http://nvdla.org/contents.html" target="_blank" rel="noopener">NVDLA Documentation</a></li>
<li><a href="https://github.com/nvdla/hw" target="_blank" rel="noopener">NVDLA Soruce Code</a></li>
</ol>

          
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
    </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cea-wind.github.io/2019/08/21/ata_reuse_in_hpconv/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SEAWIND">
    </span>
      <header class="post-header">

        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/08/21/ata_reuse_in_hpconv/" class="post-title-link" itemprop="url">AI芯片：高性能卷积计算中的数据复用</a>
              
            
          </h2>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-21 00:30:00" itemprop="dateCreated datePublished" datetime="2019-08-21T00:30:00+08:00">2019-08-21</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-31 01:06:26" itemprop="dateModified" datetime="2019-08-31T01:06:26+08:00">2019-08-31</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI-Chip/" itemprop="url" rel="index"><span itemprop="name">AI Chip</span></a></span>

                
                
              
            </span>
          

          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <blockquote>
<p>随着深度学习的飞速发展，对处理器的性能要求也变得越来越高，随之涌现出了很多针对神经网络加速设计的AI芯片。卷积计算是神经网络中最重要的一类计算，本文分析了高性能卷积计算中的数据复用，这是AI芯片设计中需要优化的重点之一，具体思路如下</p>
<ul>
<li>数据复用的动机</li>
<li>存储-计算分离框架下，针对卷积计算的优化思路</li>
<li>针对卷积计算的硬件架构设计分析</li>
<li>已经面临的挑战和解决方向</li>
<li>神经网络中数据复用的未来</li>
</ul>
</blockquote>
<h1 id="1-高性能卷积计算中数据复用的动机"><a href="#1-高性能卷积计算中数据复用的动机" class="headerlink" title="1. 高性能卷积计算中数据复用的动机"></a>1. 高性能卷积计算中数据复用的动机</h1><p>深度学习的发展过程中，较高的计算量是制约其应用的因素之一。卷积神经网络中，主要计算为三维的卷积计算（后简称为卷积），现有的主流处理器难以高性能，高效能的完成卷积计算。相比一般的通用计算，卷积计算中存在的大量数据复用以及计算的规则性，在硬件的微架构（后简称为架构）设计和计算优化上有很大的优化空间，由此诞生了众多针对深度学习加速的AI芯片。卷积计算过程可以表示如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for i = 1 : Ho</span><br><span class="line">    for j = 1 : Wo</span><br><span class="line">        for k = 1 : Co</span><br><span class="line">            for l = 1 : Hf</span><br><span class="line">                for m = 1 : Wf</span><br><span class="line">                    for n = 1 : Ci</span><br><span class="line">                        out[i,j,k] += In[i*s+l.j*s+m,n]*F[l,m,n];</span><br></pre></td></tr></table></figure>

<p>其中各参数的含义如下表</p>
<table>
<thead>
<tr>
<th>数据维度</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>Ho/Wo</td>
<td>输出feature map的高和宽</td>
</tr>
<tr>
<td>Co</td>
<td>输出的channel数目</td>
</tr>
<tr>
<td>Hf/Wf</td>
<td>filter的高和宽</td>
</tr>
<tr>
<td>Ci</td>
<td>输入的channel数目</td>
</tr>
<tr>
<td>s</td>
<td>卷积计算的stride</td>
</tr>
</tbody></table>
<p>据此可推算出输入输出数据的数据复用关系，如下表</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Mem Refs</th>
<th>Ops</th>
<th>Ops/Memory</th>
</tr>
</thead>
<tbody><tr>
<td>Filter</td>
<td>$H_f W_f C_i C_o$</td>
<td>$H_o W_o C_o H_f W_f C_i$</td>
<td>$H_oW_o$</td>
</tr>
<tr>
<td>Input</td>
<td>$H_o W_o C_i s^2$</td>
<td>$H_o W_o C_o H_f W_f C_i$</td>
<td>$H_fW_fC_o/s^2$</td>
</tr>
<tr>
<td>Output</td>
<td>$H_o W_o C_o$</td>
<td>$H_o W_o C_o H_f W_f C_i$</td>
<td>$H_f W_f C_i$</td>
</tr>
</tbody></table>
<p>可以看出，卷积计算过程中对Filter，Input和Output的数据均有很高的数据复用；充分利用计算过程中的数据复用是达到高性能和高效能的关键，这主要有以下几个方面原因</p>
<ol>
<li>利用数据复用更容易达到设计的峰值性能</li>
<li>利用数据复用可以降低内存访问，降低功耗</li>
<li>广义来看，卷积中规则的数据复用，可以降低处理器设计中缓存/控制逻辑的设计复杂度，提高总体的性能</li>
</ol>
<p>根据Roofline模型很容易解释第一点原因。Roofline模型是一种面向吞吐量的性能评价模型，它指出在理想情况下，处理器性能的理论上界。如下图（Ref. <a href="https://cs217.stanford.edu/" target="_blank" rel="noopener">CS217_Lec6</a>）所示，在数据复用不高的情况下，峰值性能受限于内存（Memory-bound）。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190824013410.png" alt></p>
<p>访问不同存储的归一化能耗比（Ref. 6）可以解释第二点原因。充分利用数据复用，使得访存多发生在RF等靠近ALU的存储上，可以极大降低功耗。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190824014114.png" alt></p>
<p>针对第三点，以AMD ZEN系列CPU为例，Die的结构如下图所示(Ref. <a href="https://en.wikichip.org/wiki/amd/microarchitectures/zen" target="_blank" rel="noopener">https://en.wikichip.org/wiki/amd/microarchitectures/zen</a>)。可以看出FPU和ALU只占CPU面积的很小一部分。大部分面积是控制逻辑的缓存。这意味着如果针对卷积计算这种规则的，具有时间和空间强相关性的，且没有分支跳转的计算设计专门的硬件，可以抛开专用处理器中复杂的控制和缓存设计，减小芯片面积，提升性能，降低功耗。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190825010558.png" alt><br><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190825010540.png" alt></p>
<p>这些特性表明，卷积计算有很大的优化空间；专门针对卷积计算设计的芯片能极大的提高性能和效能 。</p>
<h1 id="2-高性能卷积的计算方法"><a href="#2-高性能卷积的计算方法" class="headerlink" title="2. 高性能卷积的计算方法"></a>2. 高性能卷积的计算方法</h1><h2 id="2-1-卷积即矩阵乘法"><a href="#2-1-卷积即矩阵乘法" class="headerlink" title="2.1 卷积即矩阵乘法"></a>2.1 卷积即矩阵乘法</h2><p>矩阵-矩阵乘法的应用已经非常广泛，很多线性代数库充分结合了矩阵计算中的数据复用关系和处理器缓存的层次设计结构，对矩阵-矩阵乘法进行了充分的优化（GEMM，通用矩阵乘法）。$C = AB + C$的矩阵乘法可以表示为   </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for i = 1 : m</span><br><span class="line">    for j = 1 : n</span><br><span class="line">        for k = 1 : t</span><br><span class="line">            C(i,j) = A(i,k)*B(k,j) + C(i,j)</span><br></pre></td></tr></table></figure>

<p>这一形式和第1节中卷积的计算方式极其类似。实际上，针对三维的Tensor进行展开（即将卷积计算中的六层循环的内三层进行合并），很容易将卷积计算转化为矩阵乘法计算(im2col)。 其中一种展开方式如下图 （Ref. <a href="https://cs217.stanford.edu/" target="_blank" rel="noopener">CS217_Lec9</a>）</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190825233055.png" alt></p>
<p>下图也给出了一个具体的例子(Ref. <a href="https://www.zhihu.com/question/28385679/answer/41121948" target="_blank" rel="noopener">在 Caffe 中如何计算卷积？</a>，原始链接<a href="https://hal.inria.fr/file/index/docid/112631/filename/p1038112283956.pdf" target="_blank" rel="noopener">High Performance Convolutional Neural Networks for<br>Document Processing</a>)</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190825233315.png" alt></p>
<p>此时，卷积计算等价为矩阵乘法计算；而卷积计算中的数据复用关系等价为了矩阵计算中的数据复用关系。矩阵-矩阵乘的复杂度如下所示</p>
<p>Ex. | Mem Refs | Ops | Ops/Memory</p>
<p>-| - | - | -<br>Matrix-Matrix Mult | $4n^2$ | $2n^3$ | $n/2$</p>
<p>现有的处理器架构从存储和计算之间的关系上来看都是类似的，处理器计算性能的上限可以通过计算单元及其运行频率确定。为了高性能的完成矩阵计算，即使得性能达到Roofline模型中峰值，需要对矩阵计算的三层循环进行优化。在进行分析之前，需要明确两点固有的特性</p>
<ul>
<li>处理器的存储是分层设计的，越靠近计算的单元，带宽越大，容量越小</li>
<li>Roofline模型和访存的带宽有关，如下图所示</li>
</ul>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190825235250.png" alt></p>
<p>如果矩阵存储在内存中（DDR）,直接按三层循环进行矩阵计算，那么每个周期需要访问$a,b,c$三个数，此时性能会受限于DDR带宽。将矩阵放在片上的Cache中是一个好的选择，但是片上Cache的容量往往较小。解决这一个矛盾的方法是对大的矩阵进行分块，此时矩阵计算可以表示为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">for i = 1 : MR : m  // step MR</span><br><span class="line">    for j = 1 : NR : n  // step NR</span><br><span class="line">        for k = 1 : KC : c  // step KC</span><br><span class="line">        // block-dot product in cache</span><br><span class="line">        // Csub_&#123;NR*MR&#125; += Asub_&#123;MR*KC&#125;*B_sub(KC*NR)</span><br><span class="line">        // opt. mirco kernel</span><br><span class="line">            for ir = 1:MR</span><br><span class="line">                for jr = 1:NR</span><br><span class="line">                    for kr = 1: KC</span><br><span class="line">                        Csub(ir,jr) += Asub(ir,kr)*Bsub(kr,jr);</span><br></pre></td></tr></table></figure>

<p>采用这一分块方式，假设矩阵存储在DDR，分块后的矩阵存储在片上Cache；完成$MR\times NR\times NC$次乘法计算只需要从DDR获取$MR\times NR + NR \times NC + MR\times NC$ 个数据，合理的分块能降低对DDR的访问。<br>针对具有不同存储层次的处理器以及不同大小的矩阵计算而言，有不同的分块方式以达到峰值性能。虽然这一方法针对矩阵计算进行了很好的优化，但对于卷积计算而言，依旧存在缺陷</p>
<ul>
<li>im2col计算过程中针对Tensor进行了Reshape和Duplicate，这会增加带宽的消耗</li>
<li>转化得到的矩阵乘法和传统的矩阵计算维度相差较大，可能无法获得好的性能</li>
</ul>
<p>由于这些缺陷存在，如果没有针对硬件架构进行特殊的设计，卷积即矩阵乘法的设计思路往往无法达到峰值性能。</p>
<h2 id="2-2-卷积即卷积"><a href="#2-2-卷积即卷积" class="headerlink" title="2.2 卷积即卷积"></a>2.2 卷积即卷积</h2><p>采用矩阵-矩阵乘法进行卷积计算忽略了卷积本身的特性。卷积计算实际上是一类很特殊的计算，以一维的离散卷积为例，函数$f,g$之间的卷积可表示为<br>$$<br>(f*g)[n]=\sum_{m=-\infty}^{\infty}f[m]g[n-m]<br>$$</p>
<p>以有限长脉冲响应(Finite impulse response，FIR)滤波器作为离散卷积的一维特例，可表示为</p>
<p>$$<br>y[n]=\sum_{i=0}^{N}b_i \cdot x[n-i]<br>$$</p>
<p>一维的脉动阵列FIR滤波器的一种实现方式，其实现结构如下图（Ref. Wikipedia），采用这种结构，每输入一个$x[n]$，就能计算得到一个$y[n]$。这种特殊的数据复用是根据卷积计算的特殊性质得到的。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/imgfir_filter.jpg" alt></p>
<p>在卷积神经网络中，卷积层的计算一般也存在这种数据复用关系（除kernelsize &lt;= stide情况外）。如果能用好这一特性，不再将卷积转化为矩阵乘法，直接计算卷积，能获得更高的性能。为了达到这一目标，和分析矩阵乘法类似，对卷积的六层循环进行分析</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for i = 1 : Ho</span><br><span class="line">    for j = 1 : Wo</span><br><span class="line">        for k = 1 : Co</span><br><span class="line">            for l = 1 : Hf</span><br><span class="line">                for m = 1 : Wf</span><br><span class="line">                    for n = 1 : Ci</span><br><span class="line">                        out[i,j,k] += In[i*s+l.j*s+m,n]*F[l,m,n];</span><br></pre></td></tr></table></figure>

<p>类似GEMM，最内层循环会被并行化，为了达到更好的性能，可以对循环顺序进行调整（Ref.9）</p>
<ul>
<li>为了提高并行性，由于Co循环的各个计算之间相互独立，将其放置到最内层循环</li>
<li>由于Wf,Hf可能会很小，为了保证并行性，将Wo放到Co循环外</li>
</ul>
<p>完成调整之后，对卷积计算进行分块（Ref. 9）</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190827012045.png" alt></p>
<p>为了进一步提高访存的效率，Input/Ouput/Filter在内存中的排布方式也需要进行相应的调整，可参见参考文献9，此处不再描述。</p>
<p>无论是将卷积转化为矩阵计算，或者直接计算卷积，都能够通过分块的方式相对高效的完成卷积计算；由于将卷积转化为矩阵计算有一定开销，其性能可能会受到一定影响，但矩阵计算具有更高的灵活性。</p>
<h1 id="3-高性能卷积的架构设计"><a href="#3-高性能卷积的架构设计" class="headerlink" title="3. 高性能卷积的架构设计"></a>3. 高性能卷积的架构设计</h1><h2 id="3-1-GEMM加速器"><a href="#3-1-GEMM加速器" class="headerlink" title="3.1 GEMM加速器"></a>3.1 GEMM加速器</h2><p>显然，实现一个硬的GEMM加速器可以加速矩阵计算，进而加速卷积计算。针对GEMM的分块矩阵（即内层循环）进行硬件加速是一种简单，高效且相对灵活的选择，其优势包括</p>
<ul>
<li>简单，计算单元实现GEMM的mirco kernel即可，易于集成</li>
<li>灵活，支持所有可用GEMM加速的算法（理论上）</li>
<li>编译器设计和调度有很多参考实现</li>
</ul>
<p>类似设计包括Nvidia的Volta架构，其加速核心被称为Tensor Core；以及华为的达芬奇架构（Davinci Core），其卷积的计算核心被称为CUBE Core。</p>
<p>其中，Nvidia的每个Tensor Core是一个$4\times4\times4$ 的MAC阵列。计算$C = AB + C$ 时，矩阵$A,B$ 中的元素广播到4个不同的MAC上，同时每个四个乘法的结构累加到一起，如下图所示（Ref.<a href="https://devblogs.nvidia.com/tensor-core-ai-performance-milestones/" target="_blank" rel="noopener">Volta Tensor Core GPU Achieves New AI Performance Milestones</a>）</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190825001110.png" alt></p>
<p>达芬奇中的CUBE Core是一个$16\times16\times16$的MAC阵列（以Davinci Max为例），如下图所示（hotchips31），具有更高的数据复用关系。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190825005251.png" alt></p>
<p>Nvidia Volta架构中，Tensor Core仅仅只是一个特殊的计算单元，其地位和FP计算单元一致。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190821020429.png" alt></p>
<p>为了保证计算单元之外设计的统一性，送入计算单元的数据位宽完全一致，和普通的FP32计算单元比，Tensor Core在算力上有很大的优势。由下表可以看出，由于数据复用和精度的降低，Tesnor Core的理论性能是FP32的8倍（同频，其中两倍受益于精度的降低）。</p>
<table>
<thead>
<tr>
<th>Ex.</th>
<th>DataWidth</th>
<th>Data Num</th>
<th>Ops</th>
</tr>
</thead>
<tbody><tr>
<td>FP32</td>
<td>256</td>
<td>8</td>
<td>16</td>
</tr>
<tr>
<td>Tensor Core</td>
<td>256</td>
<td>16</td>
<td>128</td>
</tr>
</tbody></table>
<p>Davinci Core进行了更多的设计，配合完成高性能的卷积计算。MTE中的img2col表明其进行了3D Tensor到Matrix的转换。(Ref. <a href="https://www.jiqizhixin.com/articles/2019-08-20-4" target="_blank" rel="noopener">华为在hotchips详细介绍了达芬奇架构</a>)</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190825003826.png" alt></p>
<p>前文提到“卷积即矩阵乘法的设计思路无法达到峰值性能”，但有了硬件架构的联合设计，这一结论不再成立。譬如在Davinci Core中，在L0 Buffer进行Img2col可以降低由于im2col增加的访存带宽，合理设计的L0 BufferA/B/C也能应对卷积操作中大量的中间结果。</p>
<h2 id="3-2-脉动阵列"><a href="#3-2-脉动阵列" class="headerlink" title="3.2 脉动阵列"></a>3.2 脉动阵列</h2><p>脉动阵列的典型代表是Google TPU，Google TPU中设计的脉动阵列也是针对矩阵乘法设计（虽然有的脉动阵列也可直接计算卷积，但TPU并没有采用这一类设计）。之前的文章有对Google TPU进行细致的分析（<a href="https://cea-wind.github.io/2019/08/02/TPU_c1/">动手写一个简单版的谷歌TPU</a>），并实现了一个小规模的基于VLIW+Vector Architecture的TPU（<a href="https://github.com/cea-wind/SimpleTPU" target="_blank" rel="noopener">SimpleTPU</a>，VLIW+向量体系结构），可以作为进一步参考。Google TPU的其计算核心为一个$256\times 256$的二维脉动阵列，如下图所示（Ref. 4）</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190826005204.png" alt></p>
<p>另一个类似的设计时Telsa（特斯拉）的Full self-driving(FSD) computer，其内部的Nerual Network Accelerator(NNA)也实现了一个大规模的矩阵乘法器（NNA也可能时直接采用广播的形式，而非脉动形式进行输入）。参考其专利中的说明，其具体实现和Google TPU略有差异。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190822005906.png" alt="Telsa FSD NNA Diagram"></p>
<p>根据描述，FSD NNA中的Wieght和Activation均通过DATA FORMATTER后送入到MATRIX PROCESSOR中。乘法的结果在MATRIX PROCESSOR中进行累加，计算出最终结果后依次向下移位输出。FSD中有两个NNA，其中MATRIX PROCESSOR的大小均为$96\times 96$，和TPU1相比，FSD的主要差异表现在</p>
<ul>
<li>计算过程中，TPU的weight保持不变，而FSD的partial sum保持不变</li>
<li>TPU中的weight保持在DRAM中（片外），FSD保留在SRAM中</li>
<li>由于FSD的weight存储在片上，为了满足输入的数据排布要求，增加了在线的Weight Formatter模块</li>
<li>FSD的输入宽度为96，是3的倍数，这是应为在Data Formatter时对kernel进行了展开（$3\times 3$的kernel中有因子3）；而TPU1的宽度为256是2的幂次，kernel的展开很有可能是时间上隐式进行的</li>
</ul>
<p>针对TPU和FSD这种在数据流中的差异，可以分为Weight Stationary，Output Stationary和No Local Reuse等(Ref. 6)。这种针对Dataflow的分类方法也涉及到数据复用，但主要针对Register File上的复用进行分析，和本文针对计算中存在的数据复用关系进行分析有所不同（一个典型的差别是Ref.6中归类为no local reuse的情况在本文认为是有数据复用的）。</p>
<h2 id="3-3-直接卷积加速器"><a href="#3-3-直接卷积加速器" class="headerlink" title="3.3 直接卷积加速器"></a>3.3 直接卷积加速器</h2><p>Eyeriss是一种直接针对卷积计算优化的加速器，和其他加速器不同，Eyeriss针对Convolution Reuse进行了优化。按Ref.6的分类标准，是一种Row Stationary。(Ref. 7)</p>
<p>由于卷积神经网络计算卷积时C方向的特殊性，Convolution Reuse仅在H和W方向存在。以kernelsiz=3，stride=1为例，卷积计算中row方向的数据复用如下图(Ref. 7)</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190827020522.png" alt></p>
<p>此时ifmap中的元素3被利用了三次，只需要从存储中访问一次元素3，就能完成3次计算。当扩展为二维时，有(Ref. 7)</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190827020814.png" alt></p>
<p>即ifmap的row方向在不同PE之间进行复用；而实际上ifmap的col方向的数据会暂存在PE内的RF上，col方向的数据也在RF上进行复用。</p>
<p>Eyeriss利用了convolution resue，同时也可以利用其他resue的手段。譬如上图中的filter在水平PE之间复用，psum在垂直的若干个PE之间复用。这意味这Eyeriss相比其他结构做到了更多的数据复用，可以进一步降低功耗。</p>
<h2 id="3-4-基于变化域的卷积计算"><a href="#3-4-基于变化域的卷积计算" class="headerlink" title="3.4 基于变化域的卷积计算"></a>3.4 基于变化域的卷积计算</h2><p>一维卷积的计算复杂度为$O(n^2)$，由于时域卷积等于频域相乘，对于离散信号而言，可以通过快速傅里叶变换（Fast Fourier Transform，FFT）及其逆变换将信号在时域和频域之间变换。而FFT的计算复杂度为$O(nlogn)$，当$n$取值较大时，其计算复杂度会远低于直接计算一维卷积。</p>
<p>类似的，可以考虑在其他域进行二维/三维卷积的计算；针对卷积神经网络中的加速，有</p>
<ul>
<li>FFT based Method (<a href="https://arxiv.org/abs/1312.5851" target="_blank" rel="noopener">Fast training of convolutional networks through FFTs</a>)</li>
<li>Winograd Method (<a href="https://arxiv.org/abs/1509.09308" target="_blank" rel="noopener">Fast Algorithms for Convolutional Neural Networks</a>)</li>
</ul>
<p>但若想取得一定的加速比，这些方法对卷积核的大小和步长均有要求；这一类方法均难以适应卷积核日益小型化的发展趋势。</p>
<h1 id="4-轻量化网络带来的新的设计挑战"><a href="#4-轻量化网络带来的新的设计挑战" class="headerlink" title="4. 轻量化网络带来的新的设计挑战"></a>4. 轻量化网络带来的新的设计挑战</h1><p>当大多数AI芯片中的神经网络加速器还在使用AlexNet/VGG/ResNet跑benchmark时，新的网络层出不穷。一些为了运行在嵌入式设备上而设计的轻量化网络通过压缩卷积中各个维度的计算来降低计算量，这很大程度影响了卷积计算中的数据复用关系。考虑在轻量化网络中使用的Point Wise Conv和Depth Wise Conv，以及延时受限系统中Batch=1的全连接层，有下表</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Ops/Memory</th>
</tr>
</thead>
<tbody><tr>
<td>Point-Wise Convolution Layer</td>
<td>Input: $C_o/s^2$<br>Filter: $H_oW_o$ <br>Output:$C_i$</td>
</tr>
<tr>
<td>Depth-Wise Convolution Layer</td>
<td>Input: $H_fW_f/s^2$<br>Filter: $H_oW_o$ <br>Output:$H_f W_f$</td>
</tr>
<tr>
<td>Batch=1 Fully Connect Layer</td>
<td>Input: $C_o$<br>Filter: $1$ <br>Output:$C_i$</td>
</tr>
</tbody></table>
<p>而第三节中一些典型的基于数据复用的加速其设计中，若想达到最优性能，对数据复用的要求是</p>
<table>
<thead>
<tr>
<th>Architectures</th>
<th>Input1</th>
<th>Input2</th>
<th>Output</th>
</tr>
</thead>
<tbody><tr>
<td>$16\times16\times16$ Cube Core</td>
<td>16</td>
<td>16</td>
<td>16</td>
</tr>
<tr>
<td>$256\times256$ Systolic Array</td>
<td>256</td>
<td>1350</td>
<td>256</td>
</tr>
<tr>
<td>Eyeriss V1(Theo. low bound)</td>
<td>168</td>
<td>42</td>
<td>42</td>
</tr>
</tbody></table>
<p>数据复用关系的失配会让这些加速器在运行这些特定的Layer时出现严重的效率问题。譬如TPU V1在计算$3\times3$ Depthwise Conv时效率不可能超过$9/256=3.5\%$。这样低的效率使得这些设计良好的网络带来的数量级的计算量下降变得毫无意义。</p>
<p>从根源上看，Cube Core或者Systolic Array的优化目标都是Matrix-Matrix乘法，而Batch=1得FC和Depthwise Conv更贴近于Matrix-Vector乘法，这在本质上就是不同的。即使在软件层面的优化上，这两个运算也是分别进行优化的（GEMM和GEMV）。</p>
<p>而对于芯片设计而言，已经面临的挑战是如何设计一个在多变的复用关系下均能保证较高效率的神经网络加速器。完成这一目标，至少有两种不同的设计方向</p>
<ul>
<li>具有灵活互联结构的可重构加速器</li>
<li>标量+矢量+向量的异构加速器</li>
</ul>
<h2 id="4-1-具有灵活互联结构的可重构加速器"><a href="#4-1-具有灵活互联结构的可重构加速器" class="headerlink" title="4.1 具有灵活互联结构的可重构加速器"></a>4.1 具有灵活互联结构的可重构加速器</h2><p>上述的几乎所有的讨论和设计，都能归结到降低数据带宽上。一旦数据带宽降低后，其灵活性就受到了很大的限制，这是无法计算GEMV的原因之一。如果需要加速器能够在各种情况下有良好的表现，最直接的解决方案就是在设计上提供高的带宽和灵活性。<br>Eyeriss V2就采用这种思路对Eyeriss V1进行改进，以高效支持MobileNet。为了提供高带宽，Eyeriss设计了层次化2D Mesh的Noc（Ref. 8）</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190827225928.png" alt></p>
<p>设计中一个PE Cluster具有12个PE，12个PE之间互相连接；PE Cluster通过一个2D Mesh进行互联。Global Buffer到PE之间具有丰富的连线，在不同计算模式下可以以Multicast，broadcast和unicast进行数据传输，充分满足不同计算下的数据需求。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190827021934.png" alt></p>
<p>尽管Eyeriss V2中并没有提到可重构，但其Noc在计算不同网络时有不同的选通路径，和可重构的思想一致。一般谈到硬件可重构，一般会想到FPGA（Field－Programmable Gate Array，现场可编程大规模逻辑门阵列）。通过FPGA片上丰富的的互联结构和查找表，FPGA理论上可以用于实现各种形式的电路。</p>
<p>FPGA可以创造出各种可能，但FPGA设计上有很多冗余，在布局布线和加载上也会花费较多时间。究其本因，是因为FPGA采用了很底层的基本单元来构建整个电路。与FPGA这种细粒度可重构相对应的是粗粒度可重构网络，粗细度可重构不像FPGA一样可以控制每一个bit的选通，而是提供了一系列基本的计算单元和互联结构。根据算法的需求，可以采用不同的方式构建这些计算单元。譬如Plasticine中的PCU中，每个FU由前向，反馈等多种路径，可以构建不同的计算单元。（Ref. 11）</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190828004225.png" alt></p>
<p>多个PCU之间通过交换网络和其他PCU及PMU（Memory）相连，可以配合完成不同的操作。这些都以增加片上各个单元之间的互联性为基础。（Ref. 11）</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190828004501.png" alt></p>
<p>采用可重构设计的还有清华大学微电子系设计的Thinker芯片，具体可参考A High Energy Efficient Reconfigurable Hybrid Neural Network Processor for Deep Learning Applications（Ref11）。这样的具有灵活互联结构的可重构加速器，可以支持矩阵-矩阵乘法，矩阵-向量乘法以及更多的其他计算，具有较强的灵活性。</p>
<h2 id="4-2-标量-矢量-向量的异构加速器"><a href="#4-2-标量-矢量-向量的异构加速器" class="headerlink" title="4.2 标量+矢量+向量的异构加速器"></a>4.2 标量+矢量+向量的异构加速器</h2><p>另一种思路是对某些运算做到极致的支持，其他的运算通过CPU或者其他的Core来计算；从华为达芬奇架构，到Google TPU2/3的设计上，都有的体现，华为的达芬奇架构可参见3.1节，Google V2/3的框图如下</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190827014916.png" alt></p>
<p>Google的框图并没有透露太多的细节，仅仅表面TPU中由于MXU和scalar/vector units；华为的达芬奇架构则指出了</p>
<ul>
<li>Vector：2048bit vector with special functions（activation functions，NMS，ROI，SORT）</li>
<li>Cube：4096 FP16 MACs</li>
</ul>
<p>虽然Vector Unit似乎并不能计算FC Layer或者Depthwise Conv Layer，但这也代表了一种设计方向。当采用3D Matrix Unit进行矩阵计算时，随着$N$取值的增加，数据输入带宽随$N^2$增长，而MACs的数量随$N^3$增长，这表面针对特定的计算进行优化结果可能会更优。当然，这样的设计也有一些缺陷</p>
<ul>
<li>不同计算单元之间负载可能不均衡，导致计算出现瓶颈</li>
<li>不同计算单元之间的数据传输，调度会变得复杂</li>
</ul>
<p>当然，这种设计思想并不意味一个加速Core只能加速很受限的计算类型，依旧可以对单个的加速Core进行兼容设计，本节的讨论和4.1并不完全冲突。譬如依旧有一些简单方法可以让一个三维的MAC阵列（类似Cube Core）同时支持GEMM和GEMV操作，但依旧会有一些限制，由于数据复用的不同，这两类运算始终落在Roofline模型的不同位置。</p>
<p>但不管从哪个角度去解决面临的新的挑战，暂时都没有看到一个完美的解决方案。或许我们并不需要一个真正完美的解决方案，能解决一部分问题，创造价值就已经足够了。这里引用Cloud TPU（TPU V2/3）的一些说明</p>
<blockquote>
<p>Cloud TPU适合以下工作负载</p>
<ul>
<li>由矩阵计算主导的模型</li>
<li>主循环内没有自定义TensorFlow操作的模型</li>
<li>需要训练数周或数月的模型</li>
<li>有效批量非常大的大型和极大型模型     </li>
</ul>
<p>Cloud TPU不适合以下工作负载</p>
<ul>
<li>需要频繁分支或逐项代数主导的现有代数程序</li>
<li>稀疏方式访问内存的工作负载可能不适用于TPU</li>
<li>需要高精度算法的工作负载</li>
</ul>
</blockquote>
<p>显然，Cloud TPU聚焦在大型和极大型模型的加速上，轻量化模型不在考虑范围之内；Cloud TPU甚至明确指出了暂不支持Depthwise Convolution计算。但这些都不影响Cloud TPU的应用和创造价值。</p>
<p>而正因为没有完美的设计，选择和取舍会变得尤为重要。</p>
<h1 id="5-神经网络中数据复用的展望"><a href="#5-神经网络中数据复用的展望" class="headerlink" title="5. 神经网络中数据复用的展望"></a>5. 神经网络中数据复用的展望</h1><p>虽然在AI芯片设计过程中，都针对算法进行了深度的优化；同时算法也针对芯片实现进行了定点化和低比特量化等工作：这些看似是一个联合优化的过程。但是归根到底，既然是选择为AI应用设计芯片，在设计过程中，算法永远占主导地位。从长远来看，还需要把握算法的发展趋势。</p>
<p>站在硬件设计的角度来看，由于在计算机诞生之初就有了高性能并行计算的需求，很多设计实际上以及出现了很长时间，譬如数据并行的向量体系结构，VLIW，脉动阵列，稀疏计算都有40年以上的历史，2D Mesh互联也至少有30年。从设计上看，新的东西并不是很多，架构成功的关键在于是否顺应了时代的潮流，包括算法/需求的发展和底层硬件技术的发展。</p>
<p>在发展过程中算法和现有的硬件上的分歧往往是存在的，以本文讨论的卷积计算的数据复用为例，这和网络稀疏化的发展方向相悖。如果以极度稀疏化为目标，那么网络计算过程中的数据复用会越来越低。</p>
<p>神经网络中数据复用的未来如何，完全取决于算法的发展。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://cs217.stanford.edu/" target="_blank" rel="noopener">Hardware Accelerators for Machine Learning (CS 217)</a></li>
<li><a href="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf" target="_blank" rel="noopener">volta-architecture-whitepaper.pdf</a> </li>
<li>Scalable unified architecture for Neural Network computing (Hotchips 31)</li>
<li>In-Datacenter Performance Analysis of a Tensor Processing Unit</li>
<li>ACCELERATED MATHEMATICAL ENGINE（Telsa，US20190026078）</li>
<li>V. Sze, T.-J. Yang, Y.-H. Chen, J. Emer, “Efficient Processing of Deep Neural Networks: A Tutorial and Survey,” Proceedings of the IEEE, vol. 105, no. 12, pp. 2295-2329, December 2017</li>
<li>Y.-H. Chen, T. Krishna, J. Emer, V. Sze, “Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks,” IEEE Journal of Solid State Circuits (JSSC), ISSCC Special Issue, Vol. 52, No. 1, pp. 127-138, January 2017. </li>
<li>Y.-H. Chen, T.-J Yang, J. Emer, V. Sze, “Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices,” to appear in IEEE Journal on Emerging and Selected Topics in Circuits and Systems (JETCAS), June 2019. </li>
<li>Zhang J , Franchetti F , Low T M . High Performance Zero-Memory Overhead Direct Convolutions[J]. 2018.</li>
<li><a href="https://www.jiqizhixin.com/articles/2019-08-20-4" target="_blank" rel="noopener">华为在hotchips详细介绍了达芬奇架构</a></li>
<li>Prabhakar R, Zhang Y, Koeplinger D, et al. Plasticine: A Reconfigurable Architecture For Parallel Paterns[C]// Acm/ieee International Symposium on Computer Architecture. 2017.</li>
<li>A High Energy Efficient Reconfigurable Hybrid Neural Network Processor for Deep Learning Applications[J]. IEEE Journal of Solid-State Circuits, 2018, 53(4):968-982.</li>
</ol>

          
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
    </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cea-wind.github.io/2019/08/03/TPU_c6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SEAWIND">
    </span>
      <header class="post-header">

        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/08/03/TPU_c6/" class="post-title-link" itemprop="url">神经网络加速器应用实例：图像分类</a>
              
            
          </h2>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-03 03:56:17 / 修改时间：17:27:28" itemprop="dateCreated datePublished" datetime="2019-08-03T03:56:17+08:00">2019-08-03</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/TPU/" itemprop="url" rel="index"><span itemprop="name">TPU</span></a></span>

                
                
              
            </span>
          

          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <blockquote>
<p>深度学习飞速发展过程中，人们发现原有的处理器无法满足神经网络这种特定的大量计算，大量的开始针对这一应用进行专用芯片的设计。谷歌的张量处理单元（Tensor Processing Unit，后文简称TPU）是完成较早，具有代表性的一类设计，基于脉动阵列设计的矩阵计算加速单元，可以很好的加速神经网络的计算。本系列文章将利用公开的TPU V1相关资料，对其进行一定的简化、推测和修改，来实际编写一个简单版本的谷歌TPU，以更确切的了解TPU的优势和局限性。</p>
<ul>
<li><a href="https://cea-wind.github.io/2019/08/02/TPU_c1/">谷歌TPU概述和简化</a></li>
<li><a href="https://cea-wind.github.io/2019/08/02/TPU_c2/">TPU中的脉动阵列及其实现</a></li>
<li><a href="https://cea-wind.github.io/2019/08/02/TPU_c3/">神经网络中的归一化和池化的硬件实现</a></li>
<li><a href="https://cea-wind.github.io/2019/08/02/TPU_c4/">TPU中的指令并行和数据并行</a></li>
<li><a href="https://cea-wind.github.io/2019/08/02/TPU_c5/">Simple TPU的设计和性能评估</a></li>
<li><a href="https://cea-wind.github.io/2019/08/02/TPU_c6/">SimpleTPU实例：图像分类</a></li>
</ul>
</blockquote>
<h1 id="1-不仅仅是硬件的AI-Inference"><a href="#1-不仅仅是硬件的AI-Inference" class="headerlink" title="1. 不仅仅是硬件的AI Inference"></a>1. 不仅仅是硬件的AI Inference</h1><p><a href="https://cea-wind.github.io/2019/08/02/TPU_c5/">Simple TPU的设计和性能评估</a>中，一个神经网络加速器的硬件雏形已经搭建完成了；在<a href="https://github.com/cea-wind/SimpleTPU" target="_blank" rel="noopener">https://github.com/cea-wind/SimpleTPU</a> 上给出了相应的代码，和RTL仿真结果。在<a href="https://cea-wind.github.io/2019/08/02/TPU_c2/">TPU中的脉动阵列及其实现</a>和<a href="https://cea-wind.github.io/2019/08/02/TPU_c3/">神经网络中的归一化和池化的硬件实现</a>中，针对硬件实现中的关键模块也进行了仿真分析。但是，最终并没有给出一个可以实际运行的例子。这意味着，即使将这一部分代码应用到FPGA上，或者是实现在ASIC上后，也只有纸面性能却并不可用。</p>
<p>和很多其他的硬件设计不同，以Xilinx的AI Inference 解决方案为例（即之前的深鉴科技），用于AI Inference的设计需要考虑神经网络计算中的多样性，神经网络加速器是一个软件+硬件的解决方案。Xilinx叙述如下图<a href="https://www.xilinx.com/products/design-tools/ai-inference.html" target="_blank" rel="noopener">原始链接</a>。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803161141.png" alt></p>
<p>从上往下看，这一套解决方案包括</p>
<ul>
<li>主流的神经网络的框架的支持，包括caffe、Tensorflow和mxnet</li>
<li>提供模型压缩和优化的工具，以期在硬件上又更好的效能</li>
<li>提供模型量化的功能，使得浮点模型转化为定点模型</li>
<li>提供了Compiler，将模型映射为二进制指令序列</li>
<li>和Compiler相结合的Hardware</li>
</ul>
<p>这意味着想真正使用之前设计的神经网络加速器——SimpleTPU，还需要软件的配合。即便模型压缩不在考虑范围内，也需要将模型量化为int8精度（SimpleTPU只支持int8乘法），同时利用Compiler生成指令序列。受限于个人能力，由于配套软件的缺失，下面的例子中的量化和指令均由手工生成。也正是由于这一原因，网络结构会尽可能简单，仅以保证本系列文章完整性为目的。</p>
<h1 id="2-MLP分类实例"><a href="#2-MLP分类实例" class="headerlink" title="2. MLP分类实例"></a>2. MLP分类实例</h1><p>利用MLP对MNIST数据集进行手写数字分类的网络结构定义如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class MLP(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.hidden = nn.Linear(784,64)</span><br><span class="line">        self.fc = nn.Linear(64,10)</span><br><span class="line"> </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = x.view(-1,784)</span><br><span class="line">        x = self.hidden(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        return F.log_softmax(x, dim=1)</span><br></pre></td></tr></table></figure>

<p>生成指令后将其作为SimpleTPU的输入，并对其进行RTL仿真（testbench已经写好，直接运行即可），仿真结果如下图所示</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803161821.png" alt></p>
<p>前16张图的分类结果如下图所示</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803161851.png" alt></p>
<p>根据计算结果，可以分析得到其效率为84%。（去除了13K个用于读取图片和写回结果的时间，实际应用中，这一事件也会被计算时间覆盖）</p>
<table>
<thead>
<tr>
<th>LOC</th>
<th>Layers</th>
<th>Nonlinear function</th>
<th>Weights</th>
<th>Batch Size</th>
<th>% of Deployed</th>
</tr>
</thead>
<tbody><tr>
<td>10</td>
<td>2 FC</td>
<td>Relu</td>
<td>5M</td>
<td>512</td>
<td>16%</td>
</tr>
</tbody></table>
<p>作为参考，谷歌TPU中的数值为（尽管Simple TPU效率较高，但由于规模不同，无法直接对比效率；由于SimpleTPU完全按TPU设计，实际性能不可能高于TPU）</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803161808.png" alt></p>
<h2 id="2-1-MLP运行分析"><a href="#2-1-MLP运行分析" class="headerlink" title="2.1 MLP运行分析"></a>2.1 MLP运行分析</h2><p>通过仿真波形，可以更直观的看出SimpleTPU的运行状态。下图中，读取Weight、乘加运算单元和Pooling共同工作可以反应TPU中的指令并行和数据并行中提到的指令并行。(由上倒下的ap_start分别是MXU，POOL，LOAD WEIGHT和INSTR FETCH的工作指示信号，同时拉高代表同时工作)</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803161933.png" alt></p>
<p>观察MXU内部的信号，可以看到计算过程中的数据并行（一条指令控制多组数据，且一个周期完成多组计算）。MXU每个周期都输出psum取值，一共有32个psum，计算一个psum需要32次乘加计算。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803161950.png" alt></p>
<p>SimpleTPU为什么不够快（效率并没有接近100%）？这一问题可有下面的仿真波形看出（每次MXU启动前后都有若干个周期没有输出有效结果）</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803162002.png" alt></p>
<p>由于每次MXU执行一条向量计算指令会又若干个空闲的周期（超过64个周期，损失了10%以上的性能），导致了SimpleTPU在这一个网络上性能只有84%。MXU在启动之前需要32个周期来填满整个脉动阵列，而在输入结束后还需要32个周期来输出最后的结果。当采用HLS编写代码是，难以以这一控制力度来优化MXU的性能。如果采用Verilog HDL或者VHDL，可以采用指令之间的流水设计来消除这一延时。</p>
<h1 id="3-CNN"><a href="#3-CNN" class="headerlink" title="3. CNN"></a>3. CNN</h1><p>由于手工对神经网络进行量化和layer间融合以及生成指令的复杂性，基于CNN的图像分类/分割网络的运行实例被无限期暂停了。</p>
<p>但是一个卷积计算的实例已经在<a href="https://cea-wind.github.io/2019/08/02/TPU_c2/">TPU中的脉动阵列及其实现</a>中给出，证明了SimpleTPU计算卷积的能力。</p>
<p>根据<a href="https://cea-wind.github.io/2019/08/02/TPU_c5/">Simple TPU的设计和性能评估</a>给出的特性，SimpleTPU可以高效支持绝大多数Operator，完成计算机视觉中的多种任务。当然，最大的缺陷在于SimpleTPU不显式支持ResNet，无法直接计算residual connection中的加法（可以进行channel concatenate之后再利用一次乘加计算间接支持resnet）。</p>

          
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
    </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cea-wind.github.io/2019/08/03/TPU_c5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SEAWIND">
    </span>
      <header class="post-header">

        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/08/03/TPU_c5/" class="post-title-link" itemprop="url">Simple TPU的设计和性能评估</a>
              
            
          </h2>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-03 02:56:17 / 修改时间：17:25:44" itemprop="dateCreated datePublished" datetime="2019-08-03T02:56:17+08:00">2019-08-03</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/TPU/" itemprop="url" rel="index"><span itemprop="name">TPU</span></a></span>

                
                
              
            </span>
          

          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <blockquote>
<p>深度学习飞速发展过程中，人们发现原有的处理器无法满足神经网络这种特定的大量计算，大量的开始针对这一应用进行专用芯片的设计。谷歌的张量处理单元（Tensor Processing Unit，后文简称TPU）是完成较早，具有代表性的一类设计，基于脉动阵列设计的矩阵计算加速单元，可以很好的加速神经网络的计算。本系列文章将利用公开的TPU V1相关资料，对其进行一定的简化、推测和修改，来实际编写一个简单版本的谷歌TPU，以更确切的了解TPU的优势和局限性。</p>
</blockquote>
<h1 id="1-完成SimpleTPU的设计"><a href="#1-完成SimpleTPU的设计" class="headerlink" title="1. 完成SimpleTPU的设计"></a>1. 完成SimpleTPU的设计</h1><p>在<a href="https://cea-wind.github.io/2019/08/02/TPU_c1/">谷歌TPU概述和简化</a>中给出过SimpleTPU的框图，如下图所示。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803170350.png" alt></p>
<p>在<a href="https://cea-wind.github.io/2019/08/02/TPU_c2/">TPU中的脉动阵列及其实现</a>中介绍了矩阵/卷积计算中的主要计算单元——乘加阵列（上图4），完成了该部分的硬件代码并进行了简单的验证；在<a href="https://cea-wind.github.io/2019/08/02/TPU_c3/">神经网络中的归一化和池化的硬件实现</a>中介绍了卷积神经网络中的归一化和池化的实现方式（上图6），同时论述了浮点网络定点化的过程，并给出了Simple TPU中量化的实现方式，完成了该部分的硬件代码并进行了验证。</p>
<p>在<a href="https://cea-wind.github.io/2019/08/02/TPU_c4/">TPU中的指令并行和数据并行</a>中对整个处理单元的体系结构进行了分析和论述，包括指令并行和数据并行两个方面。那么，如何在<a href="https://cea-wind.github.io/2019/08/02/TPU_c4/">TPU中的指令并行和数据并行</a>中提到的设计思路下，将<a href="https://cea-wind.github.io/2019/08/02/TPU_c2/">TPU中的脉动阵列及其实现</a>和<a href="https://cea-wind.github.io/2019/08/02/TPU_c3/">神经网络中的归一化和池化的硬件实现</a>中提到的计算单元充分的利用，是完成Simple TPU设计的最后一步。根据SimpleTPU的框图可知，需要实现的功能包括</p>
<ul>
<li>指令的取指和译码（上图4）</li>
<li>Weight的读取（上图2）</li>
<li>各个执行单元的控制和调度（上图1）</li>
<li>读取图像和写回结果（上图5）</li>
</ul>
<p>在SimpleTPU的设计中，指令的取指和译码和Weight的读取功能都较为简单，可直接参照代码。</p>
<p>在对各个执行单元进行控制和调度时需要确保各个单元可以共同执行，没有相互之间的数据依赖关系。</p>
<p> 除此之外，还需要单独实现读取图像和写回结果的功能。SimpleTPU中只关注核心的计算功能，该部分功能并未进行优化，后续对实现效果进行分析时，也会将该部分运行时间排除在外。</p>
<p>至此，Simple TPU的设计基本完成了，代码可参见<a href="https://github.com/cea-wind/SimpleTPU" target="_blank" rel="noopener">https://github.com/cea-wind/SimpleTPU</a>。</p>
<h1 id="2-SimpleTPU的特性"><a href="#2-SimpleTPU的特性" class="headerlink" title="2. SimpleTPU的特性"></a>2. SimpleTPU的特性</h1><p>SimpleTPU的主要特性包括</p>
<ul>
<li>支持INT8乘法，支持INT32的累加操作</li>
<li>采用VLIW进行指令并行</li>
<li>采用向量体系结构进行数据并行</li>
</ul>
<p>SimpleTPU依照Google TPU V1的设计思路，可以完成神经网络推理过程中的大部分运算。依据设计，支持的运算包括（理论）</p>
<table>
<thead>
<tr>
<th>Operate</th>
<th>Support</th>
</tr>
</thead>
<tbody><tr>
<td>Conv3d</td>
<td>in_channels: Resource Constrained  <br> out_channels: Resource Constrained<br>kerner_size: Support<br>stride: support<br>padding: Support<br>dilation:Support<br>groups: Architecture Constrained<br>bias    :Support</td>
</tr>
<tr>
<td>ConvTranspose3d</td>
<td>The same as above</td>
</tr>
<tr>
<td>Maxpool2d</td>
<td>kernel_size: Support <br>stride: Support<br>padding: Support</td>
</tr>
<tr>
<td>Avgpool2d</td>
<td>The same as above</td>
</tr>
<tr>
<td>Relu</td>
<td>Only support Relu as nonlinear function</td>
</tr>
<tr>
<td>BatchNorm2d</td>
<td>BatchNorm2d is merge with Conv or Pool when inference</td>
</tr>
<tr>
<td>Linear</td>
<td>Resource Constrained</td>
</tr>
<tr>
<td>UpscalingNearest2D</td>
<td>Support (calling Avgpool2d multiple times.)</td>
</tr>
<tr>
<td>UpscalingBilinear2D</td>
<td>Support (calling Avgpool2d multiple times.)</td>
</tr>
</tbody></table>
<p>其中，Resource Constrained代表该参数的取值范围有限，主要受限于SimpleTPU的存储设计等。由于架构设计上的问题，SimpleTPU对groupconv支持极为有限，在不合适的参数下效率可能远低于普通卷积；类似的，Google TPU也不能很好支持groupconv，并明确告知不制止depthwise conv（极度稀疏化的group conv）。</p>
<p>BatchNorm2d在推理过程中实际上时进行逐点的乘法和加法，其中加法计算可以融合到下一层或者上一层的卷积计算中进行，乘法计算可以和pooling计算融合。在SimpleTPU设计，每次完成卷积计算后，均要进行一次Pooling计算，即使网络中没有pooling层，SimipleTPU增加了一个1*1，stride=1的pooling层进行等价。</p>
<p>Upscaling操作通过pooling完成计算。这是因为在SimpleTPU中，reshape操作（支持的）是没有代价的。pooling操作可以完成双线性插值的计算，因此可以完成upscaling中的所有数值的计算。可以理解为通过pooling+reshape完成了upscaling的计算。</p>
<h1 id="3-SimpleTPU的性能"><a href="#3-SimpleTPU的性能" class="headerlink" title="3. SimpleTPU的性能"></a>3. SimpleTPU的性能</h1><p>Simple TPU设计了一个32×32的int8乘加阵列计算矩阵乘法和卷积，和一个1×32的int32乘法阵列进行池化和归一化的计算。根据Xilinx HLS工具的综合结果，在UltraScale+系列的FPGA器件上，工作频率可达500MHz。因此SimpleTPU的算力约为</p>
<p>$$32\times 32 \times 500MHz \times 2 = 1 Tops$$</p>
<p>作为对比，GoogleTPU V1的算力约为92Tops（int8），差异主要在SimpleTPU的规模为其1/64，同时在FPGA上的工作频率会低于ASIC的工作频率。</p>
<p>依据设计，SimpleTPU在适合的任务下会有很高的运行效率，TPU中的指令并行和数据并行中针对这一点又更为具体的描述。从宏观上看，SimpleTPU的各个运行单元可以流水并行的，即</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803172510.png" alt></p>
<p>而针对网络中计算量最大的全连接层和卷积层，针对性设计的乘法整列和向量计算的设计方法可以让其在每个时钟周期都完成有效的乘加计算；这意味着和CPU相比，SimpleTPU可以达到极高的效率。</p>

          
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
    </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cea-wind.github.io/2019/08/03/TPU_c4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SEAWIND">
    </span>
      <header class="post-header">

        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/08/03/TPU_c4/" class="post-title-link" itemprop="url">TPU中的指令并行和数据并行</a>
              
            
          </h2>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-03 01:56:17 / 修改时间：17:47:48" itemprop="dateCreated datePublished" datetime="2019-08-03T01:56:17+08:00">2019-08-03</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/TPU/" itemprop="url" rel="index"><span itemprop="name">TPU</span></a></span>

                
                
              
            </span>
          

          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <blockquote>
<p>深度学习飞速发展过程中，人们发现原有的处理器无法满足神经网络这种特定的大量计算，大量的开始针对这一应用进行专用芯片的设计。谷歌的张量处理单元（Tensor Processing Unit，后文简称TPU）是完成较早，具有代表性的一类设计，基于脉动阵列设计的矩阵计算加速单元，可以很好的加速神经网络的计算。本系列文章将利用公开的TPU V1相关资料，对其进行一定的简化、推测和修改，来实际编写一个简单版本的谷歌TPU，以更确切的了解TPU的优势和局限性。</p>
</blockquote>
<p>TPU V1定义了一套自己的指令集，虽然在介绍处理器时，往往会先谈指令集架构，但此处却把它放到了最后，这主要基于两个原因；其一在于个人的对处理器不太了解，这也是主要原因，其二在于公开资料中并没有TPU指令集的细节和TPU微架构的描述。从数据流和计算单元出发对TPU进行分析固然容易很多，但如果想理解TPU的设计思想，依旧需要回到其架构设计上进行分析。这一部分内容有些超出了我现有的能力，不当之处还请多多指正。</p>
<p>本文主要探讨从架构设计上看，TPU时如何做高性能和高效能的设计。高性能的多来自于并行，因此本文分别讨论了指令并行和数据并行的设计方法。由于论文中并未描述TPU指令集的具体设计，除特别说明外，本文关于TPU指令集的探讨均为推测；另外，SimpleTPU的指令设计并不系统/完整，此处仅阐明设计中的几种基本思想。</p>
<h1 id="1-TPU的指令集"><a href="#1-TPU的指令集" class="headerlink" title="1. TPU的指令集"></a>1. TPU的指令集</h1><p>TPU的指令集采用CISC设计，共计有十多条指令，主要的五条指令包括</p>
<ul>
<li>Read_Host_Memory 将数据从CPU的内存中读取到TPU的Unified Buffer上</li>
<li>Read_Weights 将weight从内存中读取到TPU的 Weight FIFO 上.</li>
<li>MatrixMultiply/Convolve 执行卷积或矩阵乘法操作.</li>
<li>Activate 执行人工神经网络中的非线性操作和Pooling操作（如有）</li>
<li>Write_Host_Memory 将结果从Unified Buffer写回CPU内存.</li>
</ul>
<p>从给出的五条指令可以看出，TPU的指令集设计和通用处理器有很大的不同。指令需要显示指定数据在内存和片上buffer之间搬移的过程。而执行指令（MatrixMultiply）直接指定了Buffer的地址，指令上并不能看到一系列通用寄存器。这是因为TPU本质上还是一个专用的处理芯片，其高性能和高效能都是建立在失去一定灵活性的前提下的。为了获得更高的性能，可以采用一系列的常规方法进行设计，包括</p>
<ul>
<li>指令并行，即一次性处理更多指令，让所有执行单元高效运行</li>
<li>数据并行，即一次性处理多组数据，提高性能</li>
</ul>
<p>后文会针对这两点做进一步描述，并简单讨论TPU设计中的更多其他的优化方法和方向。</p>
<h1 id="2-指令并行"><a href="#2-指令并行" class="headerlink" title="2. 指令并行"></a>2. 指令并行</h1><h2 id="2-1-Simple-TPU中的流水线"><a href="#2-1-Simple-TPU中的流水线" class="headerlink" title="2.1 Simple TPU中的流水线"></a>2.1 Simple TPU中的流水线</h2><p>为了提高吞吐率和时钟频率，处理器通常使用流水线设计，经典的五级流水线设计一般如下所示</p>
<table>
<thead>
<tr>
<th></th>
<th>clk0</th>
<th>clk1</th>
<th>clk2</th>
<th>clk3</th>
<th>clk4</th>
<th>clk5</th>
<th>clk6</th>
<th>clk7</th>
</tr>
</thead>
<tbody><tr>
<td>instruction0</td>
<td>IF</td>
<td>ID</td>
<td>EX</td>
<td>MEM</td>
<td>WB</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>instruction1</td>
<td></td>
<td>IF</td>
<td>ID</td>
<td>EX</td>
<td>MEM</td>
<td>WB</td>
<td></td>
<td></td>
</tr>
<tr>
<td>instruction2</td>
<td></td>
<td></td>
<td>IF</td>
<td>ID</td>
<td>EX</td>
<td>MEM</td>
<td>WB</td>
<td></td>
</tr>
<tr>
<td>instruction3</td>
<td></td>
<td></td>
<td></td>
<td>IF</td>
<td>ID</td>
<td>EX</td>
<td>MEM</td>
<td>WB</td>
</tr>
</tbody></table>
<p>其中，IF指取指(insturction fetch)，ID指指令译码（instruction decode），EX指执行（Execute），MEM指内存读写（Memory Access），WB指写回寄存器(Write back)。采用流水线设计可以提高性能，如果不采用流水线设计，那么instruction1需要在clk5才能开始进行IF，严重影响其性能；如果在同一周期完成IF/ID/EX/MEM/WB的功能，由于逻辑极其复杂，会严重影响工作频率。</p>
<p>TPU论文中介绍其采用四级流水线设计，Simple TPU中采用了两级流水线，来完成控制过程。</p>
<table>
<thead>
<tr>
<th></th>
<th>clk0</th>
<th>clk1</th>
<th>clk2</th>
<th>clk3</th>
<th>clk4</th>
<th>clk5</th>
<th>clk6</th>
<th>clk7</th>
</tr>
</thead>
<tbody><tr>
<td>instruction0</td>
<td>IF&amp;ID</td>
<td>EX</td>
<td>MEM</td>
<td>WB</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>instruction1</td>
<td></td>
<td>IF&amp;ID</td>
<td>EX</td>
<td>MEM</td>
<td>WB</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>instruction2</td>
<td></td>
<td></td>
<td>IF&amp;ID</td>
<td>EX</td>
<td>MEM</td>
<td>WB</td>
<td></td>
<td></td>
</tr>
<tr>
<td>instruction3</td>
<td></td>
<td></td>
<td></td>
<td>IF&amp;ID</td>
<td>EX</td>
<td>MEM</td>
<td>WB</td>
<td></td>
</tr>
</tbody></table>
<p>也认为Simple TPU内部有四级流水线，这是因为在实际执行过程中，包括了读取寄存器，执行和写回三个部分，这三个部分是流水设计的。</p>
<h2 id="2-2-超长指令字（VLIW）"><a href="#2-2-超长指令字（VLIW）" class="headerlink" title="2.2 超长指令字（VLIW）"></a>2.2 超长指令字（VLIW）</h2><p>如前文所述，Simple TPU中有两个基本的计算单元——矩阵乘法阵列和池化计算单元。除此之外，还有一些没有显式描述的执行单元，譬如载入和存储。在这一前提下，即使TPU的指令流水线做得再好，每条指令占有的周期数也不可能小于1。如果其他执行单元的执行周期数很小，此时总会有一些执行单元处于闲置状态，处理器的瓶颈会出现在指令上。为了解决这一问题，很直接的想法时每个周期发射多条指令（另一个方法时让执行单元的执行时间变长，Simple TPU通过向量体系结构设计也有这一处理）。</p>
<p>由于TPU的专用性，以及计算过程中不存在跳转和控制的原因，采用VLIW设计多发射处理器似乎是一个很适合的方式。在这一设计下，指令发射结构时固定的，而且所有的冒险可以由编译器事先检测并处理，这很大程度可以降低硬件实现的复杂度。在Simple TPU中借鉴了VLIW的思想进行设计，如下所示(示意图)</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803171553.png" alt></p>
<p>其中各个字段具体描述如下</p>
<ul>
<li>model mask 指定了当前指令运行的模块</li>
<li>load weight 指定了从内存将weight读取到SRAM的指令</li>
<li>load act. &amp; mac &amp; store result 指定了将操作数（act.）读取到寄存器，乘加阵列计算以及将结果写回到存储器的过程</li>
<li>set weight 指定了将操作数（weight）读取到计算阵列寄存器的过程</li>
<li>load act. &amp; pooling&amp; store result field指定了将操作数（act.）读取到寄存器，完成pooling和归一化计算以及将结果写回到存储器的过程</li>
</ul>
<p>VLIW的设计放弃了很多的灵活性和兼容性，同时将很多工作放到软件完成，但依旧适合在TPU这样的偏专用的处理器中使用。Simple TPU中没有对数据冲突、依赖进行任何处理，软件需要事先完成分析并进行规避。在这一设计下一条指令可以调度最多四个模块同时工作，效率得到了提升。</p>
<h1 id="3-卷积计算中的数据并行"><a href="#3-卷积计算中的数据并行" class="headerlink" title="3. 卷积计算中的数据并行"></a>3. 卷积计算中的数据并行</h1><h2 id="3-1-单指令多数据（SIMD）"><a href="#3-1-单指令多数据（SIMD）" class="headerlink" title="3.1 单指令多数据（SIMD）"></a>3.1 单指令多数据（SIMD）</h2><p>单指令多数据，故名思意是指在一条指令控制多组数据的计算。显然，TPU core的设计中采用了这样一种数据并行的方式——一条instruction控制了256*256个乘加计算单元（MatirxMultiply/Convolve）。根据指令流和数据流之间的对应关系，可以将处理器分为以下几个类别</p>
<ul>
<li>SISD，单指令流单数据流，顺序执行指令，处理数据，可以应用指令并行方法</li>
<li>SIMD，单指令流多数据流，同一指令启动多组数据运算，可以用于开发数据级并行</li>
<li>MISD，多指令流单数据流，暂无商业实现</li>
<li>MIMD，多指令流多数据流，每个处理器用各种的指令对各自的数据进行操作，可以用在任务级并行上，也可用于数据级并行，比SIMD更灵活</li>
</ul>
<p>由于TPU应用在规则的矩阵/卷积计算中，在单个处理器内部的设计上，SIMD是数据并行的最优选择。SIMD有多种实现方式，根据给出的描述（MatirxMultiply/Convolve指令接受B<em>256输入，输出B</em>256个结果），TPU中应该采用了类似向量体系结构的设计方法。</p>
<h2 id="3-2-向量体系结构"><a href="#3-2-向量体系结构" class="headerlink" title="3.2 向量体系结构"></a>3.2 向量体系结构</h2><p>如基本单元-矩阵乘法阵列所述，计算单元完成矩阵乘法计算，即向量计算。以《计算机体系结构 : 量化研究方法》给出的例子为例，如需计算</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for(int i=0;i&lt;N;i++)</span><br><span class="line">    y[i] += a*x[i];</span><br></pre></td></tr></table></figure>

<p>以MIPS为例，对于一般的标量处理器和向量处理器而言，执行的指令序列如下所示</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803171640.png" alt></p>
<p>对于卷积神经网络中的卷积操作而言，计算可以表示为（已忽略bias）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">for(int i=0;i&lt;M;i++)&#123;</span><br><span class="line">    for(int j=0;j&lt;N;j++)&#123;</span><br><span class="line">        for(int k=0;k&lt;K;k++)&#123;</span><br><span class="line">            for(int c=0;c&lt;C;c++)&#123;</span><br><span class="line">                for(int kw=0;kw&lt;KW;kw++)&#123;</span><br><span class="line">                    for(int kh=0;kh&lt;KH;kh++)&#123;</span><br><span class="line">                        result(i,j,k) += feature(i+kw,j+kh,c)*w(k,kw,kh,c);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>由于KW和KH可能为1（即卷积核的宽度和高度），而weight在计算过程中认为是固定在计算阵列内部的，因此调整循环顺序后有</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">for(int kw=0;kw&lt;KW;kw++)&#123;</span><br><span class="line">    for(int kh=0;kh&lt;KH;kh++)&#123;</span><br><span class="line">        for(int k=0;k&lt;K;k++)&#123;</span><br><span class="line">            for(int i=0;i&lt;M;i++)&#123;</span><br><span class="line">                for(int j=0;j&lt;N;j++)&#123;</span><br><span class="line">                    for(int c=0;c&lt;C;c++)&#123;</span><br><span class="line">                        result(i,j,k) += feature(i+kw,j+kh,c)*w(k,kw,kh,c);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中第一二层循环通过指令进行控制，第三层循环在计算阵列中以256并行度进行计算，指令调度；第4-6层循环按向量处理器的设计思路进行设计，通过一条指令完成三层循环的计算。为了完成循环的计算，需要设置三个向量长度寄存器，另外，由于向量在SRAM中的地址并不连续，还需要设定三个不同的步幅寄存器。参考 基本单元-矩阵乘法阵列的代码，具体为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">short ubuf_raddr_step1;</span><br><span class="line">short ubuf_raddr_step2;</span><br><span class="line">short ubuf_raddr_step3;</span><br><span class="line">short ubuf_raddr_end1;</span><br><span class="line">short ubuf_raddr_end2;</span><br><span class="line">short ubuf_raddr_end3</span><br></pre></td></tr></table></figure>

<p> 采用这样的设计，SimpleTPU中一条指令可以完成大量数据的计算，提高了数据并行度。这些数据会并行的进入到计算阵列中完成计算（可以认为是多条车道）。由于SimpleTPU中数据的读取延时是固定的（指从SRAM），因此向量化的设计较一般处理器还更为简单。</p>
<p>根据谷歌论文中的描述，TPU中有repeat fileld，但MatrixMultiply/Convolve指令长度有限，因此可能只有一组或两组向量长度寄存器和步幅寄存器，但设计思路应该类似。</p>
<h1 id="4-其他"><a href="#4-其他" class="headerlink" title="4. 其他"></a>4. 其他</h1><p>从谷歌论文中的参数来看，TPU具有极高单位功耗下性能。这一部分来自于其内核设计，正如之前的文章中所描述的</p>
<ul>
<li>采用了INT8数据类型进行计算</li>
<li>采用了脉动阵列优化计算</li>
<li>没有采用缓存，没有分支跳转，预测和数据冲突处理（编译器完成）</li>
</ul>
<p>而从本文的内容可以看出，TPU还采用了简单的指令集设计+SIMD+向量体系结构+VLIW来进一步优化单位功耗下性能；除此之外，在V2/V3中google更进一步，还利用多核和多处理器设计进一步提高了性能。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] Jouppi, Norman P. , et al. “In-Datacenter Performance Analysis of a Tensor Processing Unit.” the 44th Annual International Symposium IEEE Computer Society, 2017.<br>[2] JohnL.Hennessy, and DavidA.Patterson. Computer architecture : a quantitative approach = 计算机体系结构 : 量化研究方法/ 5th ed.</p>

          
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
    </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cea-wind.github.io/2019/08/03/TPU_c3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SEAWIND">
    </span>
      <header class="post-header">

        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/08/03/TPU_c3/" class="post-title-link" itemprop="url">神经网络中的归一化和池化的硬件实现</a>
              
            
          </h2>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-03 00:56:17 / 修改时间：15:21:22" itemprop="dateCreated datePublished" datetime="2019-08-03T00:56:17+08:00">2019-08-03</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/TPU/" itemprop="url" rel="index"><span itemprop="name">TPU</span></a></span>

                
                
              
            </span>
          

          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <blockquote>
<p>深度学习飞速发展过程中，人们发现原有的处理器无法满足神经网络这种特定的大量计算，大量的开始针对这一应用进行专用芯片的设计。谷歌的张量处理单元（Tensor Processing Unit，后文简称TPU）是完成较早，具有代表性的一类设计，基于脉动阵列设计的矩阵计算加速单元，可以很好的加速神经网络的计算。本系列文章将利用公开的TPU V1相关资料，对其进行一定的简化、推测和修改，来实际编写一个简单版本的谷歌TPU，以更确切的了解TPU的优势和局限性。</p>
</blockquote>
<p>TBD</p>

          
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
    </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cea-wind.github.io/2019/08/03/TPU_c2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SEAWIND">
    </span>
      <header class="post-header">

        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/08/03/TPU_c2/" class="post-title-link" itemprop="url">TPU中的脉动阵列及其实现</a>
              
            
          </h2>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-03 00:17:00 / 修改时间：17:14:20" itemprop="dateCreated datePublished" datetime="2019-08-03T00:17:00+08:00">2019-08-03</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/TPU/" itemprop="url" rel="index"><span itemprop="name">TPU</span></a></span>

                
                
              
            </span>
          

          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <blockquote>
<p>深度学习飞速发展过程中，人们发现原有的处理器无法满足神经网络这种特定的大量计算，大量的开始针对这一应用进行专用芯片的设计。谷歌的张量处理单元（Tensor Processing Unit，后文简称TPU）是完成较早，具有代表性的一类设计，基于脉动阵列设计的矩阵计算加速单元，可以很好的加速神经网络的计算。本系列文章将利用公开的TPU V1相关资料，对其进行一定的简化、推测和修改，来实际编写一个简单版本的谷歌TPU，以更确切的了解TPU的优势和局限性。</p>
</blockquote>
<p>本文将对TPU中的矩阵计算单元进行分析，并给出了SimpleTPU中32×32的脉动阵列的实现方式和采用该阵列进行卷积计算的方法，以及一个卷积的设计实例，验证了其正确性。代码地址<a href="https://github.com/cea-wind/SimpleTPU/tree/master/lab1" target="_blank" rel="noopener">https://github.com/cea-wind/SimpleTPU/tree/master/lab1</a></p>
<h1 id="1-脉动阵列和矩阵计算"><a href="#1-脉动阵列和矩阵计算" class="headerlink" title="1. 脉动阵列和矩阵计算"></a>1. 脉动阵列和矩阵计算</h1><p> 脉动阵列是一种复用输入数据的设计，对于TPU中的二维脉动阵列，很多文章中构造了脉动阵列的寄存器模型，导致阅读较为困难，而实际上TPU中的二维脉动阵列设计思路十分直接。譬如当使用4×4的脉动阵列计算4×4的矩阵乘法时，有</p>
<p>$$ A\times B = \begin{bmatrix}<br>1 &amp; 1 &amp; 1 &amp; 1\ <br>2 &amp; 2 &amp; 2 &amp; 2\ <br>3 &amp; 3 &amp; 3 &amp; 3\ <br>4 &amp; 4 &amp; 4 &amp; 4<br>\end{bmatrix} \times \begin{bmatrix}<br> 1 &amp; 5 &amp; 9  &amp;13 \ <br> 2 &amp; 6 &amp; 10 &amp;14 \ <br> 3 &amp; 7 &amp; 11 &amp;15 \ <br> 4 &amp; 8 &amp; 12 &amp;16<br>\end{bmatrix} = \begin{bmatrix}<br> 10&amp; 26 &amp; 42  &amp;58 \ <br> 20&amp; 52 &amp; 84  &amp;116 \ <br> 30&amp; 78 &amp; 126 &amp;174 \ <br> 40&amp; 104&amp; 168 &amp;232<br>\end{bmatrix}$$</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803171231.png" alt></p>
<p>如上图所示，右侧是一个乘加单元的内部结构，其内部有一个寄存器，在TPU内对应存储Weight，此处存储矩阵B。左图是一个4×4的乘加阵列，假设矩阵B已经被加载到乘加阵列内部；显然，乘加阵列中每一列计算四个数的乘法并将其加在一起，即得到矩阵乘法的一个输出结果。依次输入矩阵A的四行，可以得到矩阵乘法的结果。</p>
<p>由于硬件上的限制，需要对传播路径上添加寄存器，而添加寄存器相对于在第i个时刻处理的内容变成了i+1时刻处理；这一过程可以进行计算结果上的等效。如下图所示，采用z-1代表添加一个延时为1的寄存器，如果在纵向的psum传递路径上添加寄存器，为了保证结果正确，需要在横向的输入端也添加一个寄存器（即原本在i进行乘加计算的两个数均在i+1时刻进行计算）。给纵向每个psum路径添加寄存器后，输入端处理如右图所示。（下图仅考虑第一列的处理）</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803171259.png" alt></p>
<p>当在横向的数据路径上添加寄存器时，只要每一列都添加相同延时，那么计算结果会是正确的，但是结果会在后一个周期输出，如下图所示</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803171343.png" alt></p>
<p>上述分析可以，一个4×4的乘加阵列可以计算一组4×4的乘加阵列完成计算，而对于其他维度的乘法，则可以通过多次调用的方式完成计算。譬如（4×4）×（4×8），可以将（4×8）的乘法拆分乘两个4×4的矩阵乘；而对于（4×8）×（8×4），两个矩阵计算完成后还需要将其结果累加起来，这也是为何TPU在乘加阵列后需要添加Accumulators的原因。最终脉动阵列设计如下所示（以4×4为例）</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803171414.png" alt></p>
<h1 id="2-脉动阵列的实现"><a href="#2-脉动阵列的实现" class="headerlink" title="2. 脉动阵列的实现"></a>2. 脉动阵列的实现</h1><p>如第一节所述，可通过HLS构建一个脉动阵列并进行仿真。类似TPU中的设计，采用INT8作为计算阵列的输入数据类型，为防止计算过程中的溢出，中间累加结果采用INT32存储。由于INT32的表示范围远高于INT8，认为计算过程中不存在上溢的可能性，因此没有对溢出进行处理。脉动阵列的计算结果数据类型为INT32，会在后文进行下一步处理。</p>
<p>脉动阵列实现的关键代码包括</p>
<ol>
<li>Feature向右侧移动</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">for(int j=0;j&lt;MXU_ROWNUM;j++)&#123;</span><br><span class="line">    for(int k=MXU_ROWNUM+MXU_COLNUM-2;k&gt;=0;k--)&#123;</span><br><span class="line">        if(k&gt;0)</span><br><span class="line">            featreg[j][k] = featreg[j][k-1];</span><br><span class="line">        else</span><br><span class="line">            if(i&lt;mxuparam.ubuf_raddr_num)</span><br><span class="line">                featreg[j][k] = ubuf[ubuf_raddr][j];</span><br><span class="line">            else</span><br><span class="line">                featreg[j][k] = 0;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>乘法计算以及向下方移动的psum</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">for(int j=MXU_ROWNUM-1;j&gt;=0;j--)&#123;</span><br><span class="line">    for(int k=0;k&lt;MXU_COLNUM;k++)&#123;</span><br><span class="line">        ap_int&lt;32&gt; biasreg;</span><br><span class="line">        biasreg(31,24)=weightreg[MXU_ROWNUM+0][k];</span><br><span class="line">        biasreg(23,16)=weightreg[MXU_ROWNUM+1][k];</span><br><span class="line">        biasreg(15, 8)=weightreg[MXU_ROWNUM+2][k];</span><br><span class="line">        biasreg( 7, 0)=weightreg[MXU_ROWNUM+3][k];</span><br><span class="line">        if(j==0)</span><br><span class="line">            psumreg[j][k] = featreg[j][k+j]*weightreg[j][k] + biasreg;</span><br><span class="line">        else</span><br><span class="line">            psumreg[j][k] = featreg[j][k+j]*weightreg[j][k] + psumreg[j-1][k];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>完成代码编写后可进行行为级仿真，可以看出整个计算阵列的时延关系</p>
<ol>
<li>对于同一列而言，下一行的输入比上一行晚一个周期<br><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803160407.png" alt></li>
<li>对于同一行而言，下一列的输入比上一列晚一个周期（注意同一行输入数据是一样的）<br><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803160443.png" alt></li>
<li>下一列的输出结果比上一列晚一个周期<br><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803160454.png" alt></li>
</ol>
<h1 id="3-从矩阵乘法到三维卷积"><a href="#3-从矩阵乘法到三维卷积" class="headerlink" title="3. 从矩阵乘法到三维卷积"></a>3. 从矩阵乘法到三维卷积</h1><p>卷积神经网络计算过程中，利用kh×kw×C的卷积核和H×W×C的featuremap进行乘加计算。以3×3卷积为例，如下图所示，省略Channel方向，拆分kh和kw方向分别和featuremap进行卷积，可以得到9个输出结果，这9个输出结果按照一定规律加在一起，就可以得到追后的卷积计算结果。下图给出了3×3卷积，padding=2时的计算示意图。按F1-F9给9个矩阵乘法结果编号，输出featuremap中点（2，1）——指第二行第一个点——是F1（1，1），F2（1，2），F3（1，3），F4（2，1），F5（2，2），F6（2，3），F7（3，1），F8（3，2），F9（3，3）的和。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803171137.png" alt></p>
<p>下面的MATLAB代码阐明了这种计算三维卷积的方式，9个结果错位相加的MATLAB代码如下所示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">output = out1;</span><br><span class="line">output(2:end,2:end,:) = output(2:end,2:end,:) + out2(1:end-1,1:end-1,:);</span><br><span class="line">output(2:end,:,:) = output(2:end,:,:) + out3(1:end-1,:,:);</span><br><span class="line">output(2:end,1:end-1,:) = output(2:end,1:end-1,:) + out4(1:end-1,2:end,:);</span><br><span class="line">output(:,2:end,:) = output(:,2:end,:) + out5(:,1:end-1,:);</span><br><span class="line">output(:,1:end-1,:) = output(:,1:end-1,:) + out6(:,2:end,:);</span><br><span class="line">output(1:end-1,2:end,:) = output(1:end-1,2:end,:) + out7(2:end,1:end-1,:);</span><br><span class="line">output(1:end-1,:,:) = output(1:end-1,:,:) + out8(2:end,:,:);</span><br><span class="line">output(1:end-1,1:end-1,:) = output(1:end-1,1:end-1,:) + out9(2:end,2:end,:);</span><br></pre></td></tr></table></figure>

<p>而在实际的HLS代码以及硬件实现上，部分未使用的值并未计算，因此实际计算的index和上述示意图并不相同，具体可参考testbench中的配置方法。</p>
<h1 id="4-其他"><a href="#4-其他" class="headerlink" title="4. 其他"></a>4. 其他</h1><p>GPU的volta架构中引入了Tensor Core来计算4×4的矩阵乘法，由于4×4的阵列规模较小，其内部可能并没有寄存器，设计可能类似第一节图1所示。由于其平均一个周期就能完成4×4矩阵计算，猜测采用第一节中阵列进行堆叠，如下图所示。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803160349.png" alt></p>
<p>一些FPGA加速库中利用脉动阵列实现了矩阵乘法，不过不同与TPU中将一个输入固定在MAC内部，还可以选择将psum固定在MAC内部，而两个输入都是时刻在变化的。这几种方式是类似的，就不再展开描述了。</p>

          
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
    </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cea-wind.github.io/2019/08/02/TPU_c1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SEAWIND">
    </span>
      <header class="post-header">

        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/08/02/TPU_c1/" class="post-title-link" itemprop="url">动手写一个简单版的谷歌TPU</a>
              
            
          </h2>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-02 22:34:00" itemprop="dateCreated datePublished" datetime="2019-08-02T22:34:00+08:00">2019-08-02</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-03 17:04:24" itemprop="dateModified" datetime="2019-08-03T17:04:24+08:00">2019-08-03</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/TPU/" itemprop="url" rel="index"><span itemprop="name">TPU</span></a></span>

                
                
              
            </span>
          

          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <blockquote>
<p>深度学习飞速发展过程中，人们发现原有的处理器无法满足神经网络这种特定的大量计算，大量的开始针对这一应用进行专用芯片的设计。谷歌的张量处理单元（Tensor Processing Unit，后文简称TPU）是完成较早，具有代表性的一类设计，基于脉动阵列设计的矩阵计算加速单元，可以很好的加速神经网络的计算。本系列文章将利用公开的TPU V1相关资料，对其进行一定的简化、推测和修改，来实际编写一个简单版本的谷歌TPU，以更确切的了解TPU的优势和局限性。</p>
</blockquote>
<h1 id="1-TPU设计分析"><a href="#1-TPU设计分析" class="headerlink" title="1. TPU设计分析"></a>1. TPU设计分析</h1><p>人工神经网络中的大量乘加计算（譬如三维卷积计算）大多都可以归纳成为矩阵计算。而之前有的各类处理器，在其硬件底层完成的是一个（或多个）标量/向量计算，这些处理器并没有充分利用矩阵计算中的数据复用；而Google TPU V1则是专门针对矩阵计算设计的功能强大的处理单元。参考Google公开的论文<a href="https://arxiv.org/ftp/arxiv/papers/1704/1704.04760.pdf" target="_blank" rel="noopener">In-Datacenter Performance Analysis of a Tensor Processing Unit</a>，TPU V1的结构框图如下所示</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803025510.png" alt></p>
<p>结构框图中最受瞩目的是巨大的Matrix Multiply Unit，共计64K的MAC可以在700MHz的工作频率下提供92T int8 Ops的性能。这样一个阵列进行矩阵计算的细节将会在基本单元-矩阵乘法阵列进行更进一步的阐述。TPU的设计关键在于充分利用这一乘加阵列，使其利用率尽可能高。</p>
<p>结构图中其他的部分基本都是为尽可能跑满这个矩计算阵列服务的，据此有以下设计</p>
<ul>
<li><strong>Unified Buffer 提供了256×8b@700MHz的带宽</strong>（即167GiB/s，0.25Kib×700/1024/1024=167GiB/s），以保证计算单元不会因为缺少Data in而闲置；</li>
<li><strong>Local Unified Buffer 的空间高达24MiB</strong>，这意味着计算过程的中间结果几乎无需和外界进行交互，也就不存在因为数据带宽而限制计算能力的情况；</li>
<li>Matrix Multiply Unit中<strong>每个MAC内置两个寄存器存储Weight</strong>，当一个进行计算时另一个进行新Weight的载入，以掩盖载入Weight的时间；</li>
<li>30GiB/s的带宽完成256×256Weight的载入需要大约<strong>1430个Cycles</strong>，也就意味着一组Weight至少需要计算1430Cycles，因此Accumulators的<strong>深度需要为2K</strong>（1430取2的幂次，论文中给出的数值是1350，差异未知）；</li>
<li>由于MAC和Activation模块之间需要同时进行计算，因此Accumulators需要用两倍存储来进行pingpang设计，因此<strong>Accumulators中存储的深度设计为4k</strong>；</li>
</ul>
<p>因此从硬件设计上来看，只要TPU ops/Weight Byte达到1400左右，理论上TPU就能以接近100%的效率进行计算。但在实际运行过程中，访存和计算之间的调度，读写之间的依赖关系（譬如Read After Write，需要等写完才能读），指令之间的流水线和空闲周期的处理都会在一定程度影响实际的性能。为此，TPU设计了一组指令来控制其访问存和计算，主要的指令包括</p>
<ul>
<li>Read_Host_Memory</li>
<li>Read_Weights</li>
<li>MatrixMultiply/Convolve</li>
<li>Activation</li>
<li>Write_Host_Memory</li>
</ul>
<p>所有的设计都是为了让矩阵计算单元一直处于工作状态，即希望所有其他指令可以被MatrixMultiply指令所掩盖，因此TPU采用了分离数据获取和执行的设计（Decoupled-access/execute），这意味着在发出Read_Weights指令之后，MatrixMultiply就可以开始执行，不需要等待Read_Weight指令完成；如果Weight/Activation没有准备好，matrix unit会停止。</p>
<p>需要注意的是，一条指令可以执行数千个周期，因此TPU设计过程中没有对流水线之间的空闲周期进行掩盖(存疑)，这是因为由Pipline带来的数十个周期的浪费对最终性能的影响不到1%。</p>
<p>关于指令的细节依旧不是特别清楚，更多细节有待讨论补充。</p>
<h1 id="2-TPU的简化"><a href="#2-TPU的简化" class="headerlink" title="2. TPU的简化"></a>2. TPU的简化</h1><p>实现一个完整的TPU有些过于复杂了，为了降低工作量、提高可行性，需要对TPU进行一系列的简化；为做区分，后文将简化后的TPU称为SimpleTPU。所有的简化应不失TPU本身的设计理念。</p>
<p>TPU中为了进行数据交互，存在包括PCIE Interface、DDR Interface在内的各类硬件接口；此处并不考虑这些标准硬件接口的设计，各类数据交互均通过AXI接口完成；仅关心TPU内部计算的实现，更准确的来说，Simple TPU计划实现TPU core，即下图红框所示。</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803025608.png" alt></p>
<p>由于TPU的规模太大，乘法器阵列大小为256×256，这会给调试和综合带来极大的困难，因此此处将其矩阵乘法单元修改为32×32，其余数据位宽也进行相应修改，此类修改包括</p>
<table>
<thead>
<tr>
<th>Resource</th>
<th>TPU</th>
<th>SimpleTPU</th>
</tr>
</thead>
<tbody><tr>
<td>Matrix Multiply Unit</td>
<td>256×256</td>
<td>32×32</td>
</tr>
<tr>
<td>Accumulators RAM</td>
<td>4K×256×32b</td>
<td>4K×32×32b</td>
</tr>
<tr>
<td>Unified Buffer</td>
<td>96K×256×8b</td>
<td>16K×32×8b</td>
</tr>
</tbody></table>
<ul>
<li>由于Weight FIFO实现上的困难（难以采用C语言描述）, Weight采用1K<em>32</em>8b的BRAM存放，Pingpang使用；</li>
<li>由于Matrix Multiply Unit和Accumulators之间的高度相关性，SimpleTPU将其合二为一了；</li>
<li>由于Activation和Normalized/Pool之间的高度相关性，SimpleTPU将其合二为一了（TPU本身可能也是这样做的），同时只支持RELU激活函数；</li>
<li>由于并不清楚Systolic Data Setup模块到底进行了什么操作，SimpleTPU将其删除了；SimpleTPU采用了另一种灵活而又简单的方式，即通过地址上的设计，来完成卷积计算；</li>
<li>由于中间结果和片外缓存交互会增加instruction生成的困难，此处认为计算过程中无需访问片外缓存；(这也符合TPU本身的设计思路，但由于Unified Buffer大小变成了1/24，在这一约束下只能够运行更小的模型了)</li>
<li>由于TPU V1并没有提供关于ResNet中加法操作的具体实现方式，SimpleTPU也不支持ResNet相关运算，但可以支持channel concate操作；（虽然有多种方式实现Residual Connection，但均需添加额外逻辑，似乎都会破坏原有的结构）</li>
</ul>
<p>简化后的框图如下所示，模块基本保持一致</p>
<p><img src="https://raw.githubusercontent.com/cea-wind/blogs_pictures/master/img20190803170350.png" alt></p>
<h1 id="3-基于Xilinx-HLS的实现方案"><a href="#3-基于Xilinx-HLS的实现方案" class="headerlink" title="3. 基于Xilinx HLS的实现方案"></a>3. 基于Xilinx HLS的实现方案</h1><p>一般来说，芯片开发过程中多采用硬件描述语言（Hardware Description Language, HDL），譬如Verilog HDL或者VHDL进行开发和验证。但为了提高编码的效率，同时使得代码更为易懂，SimpleTPU试图采用C语言对硬件底层进行描述；并通过HLS技术将C代码翻译为HDL代码。由于之前使用过Xilinx HLS工具，因此此处依旧采用Xilinx HLS进行开发；关于Xilinx HLS的相关信息，可以参考高层次综合（HLS）-简介，以及一个简单的开发实例利用Xilinx HLS实现LDPC译码器。</p>
<p>虽然此处选择了Xilinx HLS工具，但据我所了解，HLS可能并不适合完成这种较为复杂的IP设计。尽管SimpleTPU已经足够简单，但依旧无法在一个函数中完成所有功能，而HLS并不具有函数间相对复杂的描述能力，两个模块之间往往只能是调用关系或者通过FIFO Channel相连。但由于HLS具有<strong>易写、易读、易验证</strong>的有点，此处依旧选择了HLS作为开发语言，并通过一些手段规避掉了部分问题。真实应用中，采用HDL或者HDL结合HLS进行开发是更为合适的选择。</p>
<p>按规划之后将给出两个关键计算单元的实现，以及控制逻辑和指令的设计方法；最后将给出一个实际的神经网络及其仿真结果和分析。具体包括</p>
<ul>
<li><a href="https://cea-wind.github.io/2019/08/02/TPU_c1/">谷歌TPU概述和简化</a></li>
<li><a href="https://cea-wind.github.io/2019/08/02/TPU_c2/">TPU中的脉动阵列及其实现</a></li>
<li><a href="https://cea-wind.github.io/2019/08/02/TPU_c3/">神经网络中的归一化和池化的硬件实现</a></li>
<li><a href="https://cea-wind.github.io/2019/08/02/TPU_c4/">TPU中的指令并行和数据并行</a></li>
<li><a href="https://cea-wind.github.io/2019/08/02/TPU_c5/">Simple TPU的设计和性能评估</a></li>
<li><a href="https://cea-wind.github.io/2019/08/02/TPU_c6/">SimpleTPU实例：图像分类</a></li>
</ul>

          
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
    </footer>
  </div>
  
  
  
  </article>

    
  </section>

  


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description motion-element" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/cea-wind" title="GitHub &rarr; https://github.com/cea-wind" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
  </div>



        </div>
      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.3.0</div>

        








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
      </div>

    

  </div>

  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  <script src="/js/utils.js?v=7.3.0"></script>
  <script src="/js/motion.js?v=7.3.0"></script>

  
  <script src="/js/affix.js?v=7.3.0"></script>
  <script src="/js/schemes/pisces.js?v=7.3.0"></script>


  

  <script src="/js/next-boot.js?v=7.3.0"></script>

  

  

  


  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>



  
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  
    
      <script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  
































</body>
</html>
